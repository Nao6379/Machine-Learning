{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in c:\\users\\naoya\\anaconda3\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: future in c:\\users\\naoya\\anaconda3\\lib\\site-packages (from hyperopt) (0.18.2)\n",
      "Requirement already satisfied: six in c:\\users\\naoya\\anaconda3\\lib\\site-packages (from hyperopt) (1.15.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\naoya\\anaconda3\\lib\\site-packages (from hyperopt) (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\naoya\\anaconda3\\lib\\site-packages (from hyperopt) (1.19.5)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\naoya\\anaconda3\\lib\\site-packages (from hyperopt) (2.4)\n",
      "Requirement already satisfied: py4j in c:\\users\\naoya\\anaconda3\\lib\\site-packages (from hyperopt) (0.10.9.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\naoya\\anaconda3\\lib\\site-packages (from hyperopt) (4.47.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\naoya\\anaconda3\\lib\\site-packages (from hyperopt) (1.5.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\naoya\\anaconda3\\lib\\site-packages (from networkx>=2.2->hyperopt) (4.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir C:/Users/Naoya/Master_thesis/my_log_dir2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験参加者別"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隠れ層のチューニングあり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0453646183013916\n",
      "0.8998081088066101\n",
      "1.1225701570510864\n",
      "1.068488359451294\n",
      "1.2261911630630493\n",
      "1.143628478050232\n",
      "1.107463002204895\n",
      "1.084760069847107\n",
      "1.1520825624465942\n",
      "1.1602040529251099\n",
      "0.7829303741455078\n",
      "1.078192114830017\n",
      "1.1640106439590454\n",
      "0.6379026770591736\n",
      "1.1055513620376587\n",
      "0.760429859161377\n",
      "1.064826250076294\n",
      "1.1433197259902954\n",
      "1.0801656246185303\n",
      "1.2221111059188843\n",
      "0.7531495094299316\n",
      "0.7526265978813171\n",
      "0.7508746981620789\n",
      "0.8104107975959778\n",
      "0.7637422680854797\n",
      "0.8049700856208801\n",
      "0.8347452282905579\n",
      "0.726554811000824\n",
      "1.0134295225143433\n",
      "0.811396062374115\n",
      "1.020084023475647\n",
      "1.1384034156799316\n",
      "1.1461492776870728\n",
      "0.8772363066673279\n",
      "0.7337151169776917\n",
      "1.1438242197036743\n",
      "0.987175703048706\n",
      "1.1398696899414062\n",
      "1.140297293663025\n",
      "0.7772789001464844\n",
      "1.1409085988998413\n",
      "0.9923028945922852\n",
      "0.7287809252738953\n",
      "1.141053557395935\n",
      "0.761354923248291\n",
      "1.1504058837890625\n",
      "1.14142644405365\n",
      "1.1419885158538818\n",
      "0.958526611328125\n",
      "0.7884743809700012\n",
      "1.1537264585494995\n",
      "0.964660108089447\n",
      "1.1247252225875854\n",
      "0.9196529388427734\n",
      "1.143228530883789\n",
      "0.7373263239860535\n",
      "0.7529414296150208\n",
      "1.0349407196044922\n",
      "1.1418277025222778\n",
      "0.8362779021263123\n",
      "0.71528559923172\n",
      "0.7798669934272766\n",
      "1.1443760395050049\n",
      "0.7261379361152649\n",
      "0.8286442160606384\n",
      "0.8602110743522644\n",
      "0.8400076031684875\n",
      "0.8164894580841064\n",
      "0.7043351531028748\n",
      "0.8042705655097961\n",
      "0.8593193888664246\n",
      "0.7735161781311035\n",
      "0.670255184173584\n",
      "0.6969804763793945\n",
      "1.150276780128479\n",
      "1.0060423612594604\n",
      "0.7754270434379578\n",
      "1.0793037414550781\n",
      "0.8909445405006409\n",
      "0.7155203819274902\n",
      "0.9110286831855774\n",
      "1.1300687789916992\n",
      "0.6980834007263184\n",
      "1.0511919260025024\n",
      "0.7768505215644836\n",
      "0.9473653435707092\n",
      "0.7156763076782227\n",
      "1.0854885578155518\n",
      "0.9926199913024902\n",
      "1.1531767845153809\n",
      "1.116746425628662\n",
      "0.8862423896789551\n",
      "1.1518765687942505\n",
      "0.6809539794921875\n",
      "0.7497879862785339\n",
      "0.7238982319831848\n",
      "0.9524449706077576\n",
      "1.1130200624465942\n",
      "0.9274449944496155\n",
      "0.824072539806366\n",
      "1.1421411037445068\n",
      "1.057017207145691\n",
      "0.926657497882843\n",
      "0.8643586039543152\n",
      "0.6986686587333679\n",
      "1.1399685144424438\n",
      "1.0922502279281616\n",
      "0.8245105743408203\n",
      "0.8096268177032471\n",
      "1.1436625719070435\n",
      "0.9123167991638184\n",
      "0.9216794371604919\n",
      "0.9329133033752441\n",
      "1.1351475715637207\n",
      "0.9751842021942139\n",
      "1.1033493280410767\n",
      "0.7782326340675354\n",
      "0.751774787902832\n",
      "1.0738930702209473\n",
      "1.1435785293579102\n",
      "1.11337149143219\n",
      "1.1288264989852905\n",
      "0.7206315994262695\n",
      "1.0264970064163208\n",
      "0.9611188769340515\n",
      "0.9429739117622375\n",
      "0.9871495366096497\n",
      "0.737550675868988\n",
      "0.8753061294555664\n",
      "0.720548689365387\n",
      "0.9801211357116699\n",
      "1.1238163709640503\n",
      "0.8258636593818665\n",
      "1.150088906288147\n",
      "0.8833417296409607\n",
      "0.9413244128227234\n",
      "1.0071443319320679\n",
      "0.7773880958557129\n",
      "1.1439396142959595\n",
      "0.8155766129493713\n",
      "0.7976062893867493\n",
      "1.1365290880203247\n",
      "0.7609906792640686\n",
      "0.7533580660820007\n",
      "1.0723236799240112\n",
      "0.7726914286613464\n",
      "0.7666212916374207\n",
      "0.6472617983818054\n",
      "0.8113395571708679\n",
      "0.7567022442817688\n",
      "0.8035572171211243\n",
      "0.6639469265937805\n",
      "0.6996280550956726\n",
      "0.8837954998016357\n",
      "0.7962150573730469\n",
      "0.8890026211738586\n",
      "0.8283278346061707\n",
      "1.1462353467941284\n",
      "0.8930352330207825\n",
      "0.8999802470207214\n",
      "0.768022358417511\n",
      "1.1264437437057495\n",
      "0.7917985916137695\n",
      "0.9455347061157227\n",
      "0.8620162010192871\n",
      "0.7427212595939636\n",
      "1.146479606628418\n",
      "1.1427119970321655\n",
      "0.7839095592498779\n",
      "0.8452240824699402\n",
      "1.1494282484054565\n",
      "0.8081192970275879\n",
      "0.7855751514434814\n",
      "1.0165215730667114\n",
      "0.9395713806152344\n",
      "1.144691824913025\n",
      "0.9801809191703796\n",
      "0.7259926795959473\n",
      "0.746147871017456\n",
      "1.1641653776168823\n",
      "0.8732894062995911\n",
      "0.9846293330192566\n",
      "0.8856480717658997\n",
      "0.8009842038154602\n",
      "1.1470671892166138\n",
      "1.123055100440979\n",
      "0.8523086905479431\n",
      "0.9712464213371277\n",
      "1.1249052286148071\n",
      "0.6888148188591003\n",
      "0.8609073162078857\n",
      "0.9877731204032898\n",
      "0.9306851029396057\n",
      "1.1474454402923584\n",
      "0.959760844707489\n",
      "0.6937163472175598\n",
      "0.8049647212028503\n",
      "1.0821930170059204\n",
      "0.6947576403617859\n",
      "0.8811270594596863\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, fmin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "#ニューロン数を20にするなどしてパラメータ数を多くするとALLATRIBUTEERRORが発生する\n",
    "#hp.choiceを使ったパラメータ値のprint(best)による出力は、選択した数値ではなくあくまでインデックスの値を返すため+1する必要がある\n",
    "hyperopt_parameters = {\n",
    "    'n_hidden': hp.choice('n_hidden',[1, 2, 3, 4, 5]),\n",
    "    #'n_neurons': hp.choice('n_neurons', [[hp.quniform(f'n_neurons_{neuron}', 1, 20, 1) for neuron in range(10)]]),\n",
    "    'n_neurons': hp.choice('n_neurons', [[hp.choice(f'n_neurons_{neuron}',\n",
    "                                                    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) for neuron in range(10)]]),\n",
    "    'n_drop': hp.choice('n_drop', [[hp.uniform(f'n_drop_{drop}', 0.3, 0.5) for drop in range(10)]]),\n",
    "    'optimizer': hp.choice('optimizer', ['Adam', 'SGD', 'RMSprop']),\n",
    "    'batch_size': hp.choice('batch_size', [8, 16, 32, 64])\n",
    "    #'epochs': hp.choice('epochs', [100, 250, 500, 1000, 2000]),\n",
    "    #'learning_rate': hp.uniform('learning_rate', 0.00001, 0.01),\n",
    "}\n",
    "\n",
    "# 静止状態\n",
    "\n",
    "df = pd.read_csv('okada_K2.csv')\n",
    "df = df.sample(frac=1, random_state=0)\n",
    "X_train = df.iloc[:,:6]\n",
    "y_train = df.iloc[:,6]\n",
    "y_mean = np.mean(y_train)\n",
    "y_std = np.std(y_train)\n",
    "y_train = (y_train - y_mean)/y_std\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.1, random_state=0)\n",
    "\n",
    "def function(args):\n",
    "    #n_hidden = int(args['n_hidden'])\n",
    "    n_hidden = args['n_hidden']\n",
    "    n_drop = args['n_drop']\n",
    "    batch_size = int(args['batch_size'])\n",
    "    optimizer = args['optimizer']\n",
    "    n_neurons = args['n_neurons']\n",
    "    #print(args['n_hidden'],args['n_neurons'],args['n_drop'],args['batch_size'],args['optimizer'])\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(6,)))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(layers.Dense(n_neurons[layer], activation='relu'))\n",
    "        model.add(layers.Dropout(n_drop[layer]))\n",
    "        #print(args['nneurons'][layer],args['ndrop'][layer])\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    #model.summary()\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics='mae')\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor = 'val_mae', patience = 20)]\n",
    "    \n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              epochs=1000,\n",
    "              batch_size=batch_size,\n",
    "              verbose=0,\n",
    "              callbacks=callbacks,\n",
    "              validation_split=0.1) # 検証用のデータ\n",
    "    \n",
    "    evaluation = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    print(evaluation[0])\n",
    "    return evaluation[0]\n",
    "\n",
    "#for i in tqdm(range(200)):\n",
    " #   trials = Trials()\n",
    "  #  best = fmin(function,\n",
    "   #             hyperopt_parameters,\n",
    "    #            algo=tpe.suggest,\n",
    "     #           max_evals=1,\n",
    "      #          trials=trials,\n",
    "       #         verbose=0)\n",
    "    \n",
    "trials = Trials()\n",
    "best = fmin(function,\n",
    "            hyperopt_parameters,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=200,\n",
    "            trials=trials,\n",
    "            verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 1, 'n_drop': 0, 'n_drop_0': 0.33974299308471934, 'n_drop_1': 0.4821875483076665, 'n_drop_2': 0.355426805273229, 'n_drop_3': 0.49622233764912715, 'n_drop_4': 0.3601546899222607, 'n_drop_5': 0.35873674925518034, 'n_drop_6': 0.430105685827313, 'n_drop_7': 0.47230736500813225, 'n_drop_8': 0.41512331802483937, 'n_drop_9': 0.47198284431598236, 'n_hidden': 0, 'n_neurons': 0, 'n_neurons_0': 7, 'n_neurons_1': 7, 'n_neurons_2': 9, 'n_neurons_3': 0, 'n_neurons_4': 0, 'n_neurons_5': 9, 'n_neurons_6': 7, 'n_neurons_7': 0, 'n_neurons_8': 0, 'n_neurons_9': 8, 'optimizer': 1}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 2, 'n_drop': 0, 'n_drop_0': 0.38991779865074466, 'n_drop_1': 0.4134436274982293, 'n_drop_2': 0.3417495990777167, 'n_drop_3': 0.4218835814717209, 'n_drop_4': 0.4850267814094251, 'n_drop_5': 0.4181478551178712, 'n_drop_6': 0.30012623173534975, 'n_drop_7': 0.4876931890614151, 'n_drop_8': 0.45699944571371287, 'n_drop_9': 0.3999050494456673, 'n_hidden': 0, 'n_neurons': 0, 'n_neurons_0': 7, 'n_neurons_1': 2, 'n_neurons_2': 6, 'n_neurons_3': 9, 'n_neurons_4': 8, 'n_neurons_5': 2, 'n_neurons_6': 8, 'n_neurons_7': 4, 'n_neurons_8': 7, 'n_neurons_9': 0, 'optimizer': 1}\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 2, 'n_drop': 0, 'n_drop_0': 0.3122794447292501, 'n_drop_1': 0.3957052878635843, 'n_drop_2': 0.35618021983586834, 'n_drop_3': 0.3714288601839902, 'n_drop_4': 0.3183172877715252, 'n_drop_5': 0.4854759357005794, 'n_drop_6': 0.3451761068305333, 'n_drop_7': 0.4873889650813854, 'n_drop_8': 0.492007451771217, 'n_drop_9': 0.42836835030764514, 'n_hidden': 1, 'n_neurons': 0, 'n_neurons_0': 9, 'n_neurons_1': 9, 'n_neurons_2': 2, 'n_neurons_3': 9, 'n_neurons_4': 3, 'n_neurons_5': 8, 'n_neurons_6': 4, 'n_neurons_7': 7, 'n_neurons_8': 8, 'n_neurons_9': 4, 'optimizer': 1}\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 2, 'n_drop': 0, 'n_drop_0': 0.32390912297135954, 'n_drop_1': 0.42748606109032344, 'n_drop_2': 0.44429212718387334, 'n_drop_3': 0.4190692923066496, 'n_drop_4': 0.36459130463963446, 'n_drop_5': 0.3963518656092614, 'n_drop_6': 0.32866071240971373, 'n_drop_7': 0.41384258180894273, 'n_drop_8': 0.32998520487995453, 'n_drop_9': 0.33388615576929814, 'n_hidden': 0, 'n_neurons': 0, 'n_neurons_0': 9, 'n_neurons_1': 7, 'n_neurons_2': 9, 'n_neurons_3': 9, 'n_neurons_4': 8, 'n_neurons_5': 6, 'n_neurons_6': 3, 'n_neurons_7': 0, 'n_neurons_8': 3, 'n_neurons_9': 0, 'optimizer': 1}\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 0, 'n_drop': 0, 'n_drop_0': 0.3188286882828955, 'n_drop_1': 0.3041299943182385, 'n_drop_2': 0.3687489519724732, 'n_drop_3': 0.40700468002024565, 'n_drop_4': 0.4054258787410093, 'n_drop_5': 0.3442792676492256, 'n_drop_6': 0.4337065714422552, 'n_drop_7': 0.44967762323274013, 'n_drop_8': 0.3317729020093095, 'n_drop_9': 0.35911659800899554, 'n_hidden': 0, 'n_neurons': 0, 'n_neurons_0': 8, 'n_neurons_1': 3, 'n_neurons_2': 4, 'n_neurons_3': 2, 'n_neurons_4': 2, 'n_neurons_5': 9, 'n_neurons_6': 0, 'n_neurons_7': 5, 'n_neurons_8': 4, 'n_neurons_9': 8, 'optimizer': 1}\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 1, 'n_drop': 0, 'n_drop_0': 0.3002234397708743, 'n_drop_1': 0.3873031913493352, 'n_drop_2': 0.4190518150668984, 'n_drop_3': 0.3733095204711307, 'n_drop_4': 0.41331600553890485, 'n_drop_5': 0.3694304684383882, 'n_drop_6': 0.34437744253082014, 'n_drop_7': 0.3744783198439581, 'n_drop_8': 0.31232261743813067, 'n_drop_9': 0.34645204966275506, 'n_hidden': 0, 'n_neurons': 0, 'n_neurons_0': 4, 'n_neurons_1': 8, 'n_neurons_2': 0, 'n_neurons_3': 5, 'n_neurons_4': 9, 'n_neurons_5': 4, 'n_neurons_6': 5, 'n_neurons_7': 1, 'n_neurons_8': 2, 'n_neurons_9': 5, 'optimizer': 0}\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適なハイパーパラメータを使用して再学習\n",
    "\n",
    "num_layers = int(best_params[0])\n",
    "neurons_per_layer = [int(best_params[i]) for i in range(1, num_layers+1)]\n",
    "dropout_rate = best_params[num_layers+1]\n",
    "batch_size = int(best_params[num_layers+2])\n",
    "optimizer = ['adam', 'sgd', 'rmsprop'][int(best_params[num_layers+3])]\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(neurons_per_layer[0], activation='relu', input_shape=(6,)))\n",
    "for i in range(1, num_layers):\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(neurons_per_layer[i], activation='relu'))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mse',\n",
    "              metrics=['mae'])\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_mae', patience=20)]\n",
    "\n",
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    epochs=500,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=0,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test)\n",
    "predictions = model.predict(X_test).flatten()\n",
    "print(\"Test set loss: {}, Test set mean_squared_error: {}\".format(score[0], score[1]))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, fmin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "#ニューロン数を20にするなどしてパラメータ数を多くするとALLATRIBUTEERRORが発生する\n",
    "#hp.choiceを使ったパラメータ値のprint(best)による出力は、選択した数値ではなくあくまでインデックスの値を返すため+1する必要がある\n",
    "hyperopt_parameters = {\n",
    "    'n_hidden': hp.choice('n_hidden',[1, 2, 3, 4, 5]),\n",
    "    #'n_neurons': hp.choice('n_neurons', [[hp.quniform(f'n_neurons_{neuron}', 1, 20, 1) for neuron in range(10)]]),\n",
    "    'n_neurons': hp.choice('n_neurons', [[hp.choice(f'n_neurons_{neuron}',\n",
    "                                                    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) for neuron in range(10)]]),\n",
    "    'n_drop': hp.choice('n_drop', [[hp.uniform(f'n_drop_{drop}', 0.3, 0.5) for drop in range(10)]]),\n",
    "    'optimizer': hp.choice('optimizer', ['Adam', 'SGD', 'RMSprop']),\n",
    "    'batch_size': hp.choice('batch_size', [8, 16, 32, 64])\n",
    "    #'epochs': hp.choice('epochs', [100, 250, 500, 1000, 2000]),\n",
    "    #'learning_rate': hp.uniform('learning_rate', 0.00001, 0.01),\n",
    "}\n",
    "\n",
    "# 静止状態\n",
    "\n",
    "df = pd.read_csv('okada_K2.csv')\n",
    "df = df.sample(frac=1, random_state=0)\n",
    "X_train = df.iloc[:,:6]\n",
    "y_train = df.iloc[:,6]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.1, random_state=0)\n",
    "\n",
    "def function(args):\n",
    "    #n_hidden = int(args['n_hidden'])\n",
    "    n_hidden = args['n_hidden']\n",
    "    n_drop = args['n_drop']\n",
    "    batch_size = int(args['batch_size'])\n",
    "    optimizer = args['optimizer']\n",
    "    n_neurons = args['n_neurons']\n",
    "    #print(args['n_hidden'],args['n_neurons'],args['n_drop'],args['batch_size'],args['optimizer'])\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(6,)))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(layers.Dense(n_neurons[layer], activation='relu'))\n",
    "        model.add(layers.Dropout(n_drop[layer]))\n",
    "        #print(args['nneurons'][layer],args['ndrop'][layer])\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    #model.summary()\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics='mae')\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor = 'val_mae', patience = 20)]\n",
    "    \n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              epochs=1000,\n",
    "              batch_size=batch_size,\n",
    "              verbose=0,\n",
    "              callbacks=callbacks,\n",
    "              validation_split=0.1) # 検証用のデータ\n",
    "    \n",
    "    evaluation = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    return evaluation[0]\n",
    "\n",
    "#for i in tqdm(range(200)):\n",
    " #   trials = Trials()\n",
    "  #  best = fmin(function,\n",
    "   #             hyperopt_parameters,\n",
    "    #            algo=tpe.suggest,\n",
    "     #           max_evals=1,\n",
    "      #          trials=trials,\n",
    "       #         verbose=0)\n",
    "    \n",
    "trials = Trials()\n",
    "best = fmin(function,\n",
    "            hyperopt_parameters,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=200,\n",
    "            trials=trials,\n",
    "            verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隠れ層のチューニングなし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333bdcbb895c43798e39d2254f279bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, fmin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "hyperopt_parameters = {\n",
    "    'l1_drop': hp.uniform('l1_drop', 0.3, 0.5),\n",
    "    'l2_drop': hp.uniform('l2_drop', 0.3, 0.5),\n",
    "    'l1_out': hp.choice('l1_out',[1, 3, 5, 10, 15, 20]),\n",
    "    'l2_out': hp.choice('l2_out',[1, 3, 5, 10, 15, 20]),\n",
    "    'optimizer': hp.choice('optimizer', ['Adam', 'SGD', 'RMSprop','Adagrad','Adadelta']),\n",
    "    'batch_size': hp.choice('batch_size', [8, 16, 32, 64]),\n",
    "    'epochs': hp.choice('epochs', [100, 250, 500, 1000, 2000]),\n",
    "    #'learning_rate': hp.uniform('learning_rate', 0.00001, 0.01),\n",
    "}\n",
    "\n",
    "# 静止状態\n",
    "\n",
    "df = pd.read_csv('okada_K3.csv')\n",
    "df = df.sample(frac=1, random_state=0)\n",
    "X_train = df.iloc[:,:6]\n",
    "y_train = df.iloc[:,6]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.1, random_state=0)\n",
    "\n",
    "def function(args):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(args['l1_out'], activation='relu', input_shape=(6,)))\n",
    "    model.add(layers.Dropout(args['l1_drop']))\n",
    "    model.add(layers.Dense(args['l2_out'], activation='relu'))\n",
    "    model.add(layers.Dropout(args['l2_drop']))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(optimizer=args['optimizer'], loss='mse', metrics='mse')\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor = 'val_mse', patience = 20)]\n",
    "\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              epochs=args['epochs'],\n",
    "              batch_size=args['batch_size'],\n",
    "              verbose=0,\n",
    "              callbacks=callbacks,\n",
    "              validation_split=0.1) # 検証用のデータ\n",
    "    \n",
    "    evaluation = model.evaluate(X_test, y_test, batch_size=args['batch_size'], verbose=0)\n",
    "    return evaluation[0]\n",
    "\n",
    "#trials = Trials()\n",
    "#best = fmin(function,\n",
    " #               hyperopt_parameters,\n",
    "  #              algo=tpe.suggest,\n",
    "   #             max_evals=200,\n",
    "    #            trials=trials,\n",
    "     #           verbose=0)\n",
    "for i in tqdm(range(200)):\n",
    "    trials = Trials()\n",
    "    best = fmin(function,\n",
    "                hyperopt_parameters,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=1,\n",
    "                trials=trials,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 0, 'epochs': 4, 'l1_drop': 0.2228814672471817, 'l1_out': 3, 'l2_drop': 0.001771712353763788, 'l2_out': 1, 'optimizer': 0}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best['batch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP-EIによる最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.         16.          0.         15.         16.         11.\n",
      "  15.          3.         11.          7.         11.         12.\n",
      "   7.          0.46146048  0.31366683  0.33579516  0.41215964  0.44134846\n",
      "   0.36844898  0.36150442  0.4870921   0.36594821  0.32314823]]\n",
      "[[ 4.          8.          1.          7.          8.          6.\n",
      "   7.         10.          8.         13.          6.          1.\n",
      "  17.          0.33131622  0.4032712   0.40289095  0.34169084  0.36344491\n",
      "   0.38637462  0.32798254  0.47208793  0.48940796  0.38899528]]\n",
      "[[ 5.         16.          2.          5.         16.          3.\n",
      "  17.         10.         10.         20.         14.         10.\n",
      "  12.          0.35242363  0.45360033  0.44789597  0.32710021  0.42733789\n",
      "   0.41679138  0.41713805  0.3239864   0.49788336  0.34027487]]\n",
      "[[ 6.         16.          0.          5.          6.         17.\n",
      "  20.         13.         17.         13.          9.         17.\n",
      "   1.          0.43437289  0.31686813  0.49095937  0.33081782  0.39094645\n",
      "   0.33613595  0.42763711  0.47444659  0.46561584  0.41412436]]\n",
      "[[ 7.         64.          1.          8.         14.          2.\n",
      "   8.          6.          5.          4.         13.         18.\n",
      "   9.          0.41424726  0.48450984  0.49547384  0.34723926  0.30618554\n",
      "   0.47365007  0.4888881   0.48156387  0.45763604  0.42851703]]\n",
      "[[ 3.         32.          0.          2.         15.         20.\n",
      "   9.          1.          3.         18.          6.         10.\n",
      "   1.          0.40116081  0.35620914  0.46232119  0.31079186  0.43254766\n",
      "   0.32233038  0.40866388  0.47888555  0.37805533  0.34257946]]\n",
      "[[ 2.         32.          0.          7.         14.          3.\n",
      "   8.          5.         13.         18.         14.         18.\n",
      "   6.          0.40576161  0.40203889  0.45560657  0.37402082  0.44871477\n",
      "   0.4523325   0.31904828  0.39469748  0.41997322  0.4154406 ]]\n",
      "[[10.         32.          1.          3.         14.         20.\n",
      "   7.         12.          5.          3.          7.          4.\n",
      "   2.          0.33108335  0.31151757  0.30002077  0.37023658  0.38146569\n",
      "   0.34301693  0.44522694  0.43092768  0.44781896  0.45259628]]\n",
      "[[ 3.         32.          1.          1.         11.          5.\n",
      "   7.          1.          7.          3.         12.         16.\n",
      "   9.          0.36761791  0.49313798  0.35801324  0.31539065  0.32067044\n",
      "   0.44550482  0.42451123  0.3282188   0.34926088  0.4133733 ]]\n",
      "[[ 2.         16.          0.          3.         17.         16.\n",
      "   7.         11.          5.         18.         20.          8.\n",
      "   1.          0.46370838  0.48453373  0.34948954  0.3795506   0.34005561\n",
      "   0.30566344  0.44263413  0.42114202  0.39572298  0.41261108]]\n",
      "[[ 4.         32.          0.         13.          6.         17.\n",
      "  17.          3.         11.         20.         12.         10.\n",
      "  18.          0.47709673  0.47720283  0.33195718  0.39950922  0.3769189\n",
      "   0.47726281  0.31150988  0.43610141  0.44424061  0.41072287]]\n",
      "[[ 6.          8.          2.         17.          9.          2.\n",
      "   5.          7.         18.         10.         20.         15.\n",
      "   9.          0.41675942  0.36066768  0.31458639  0.47700295  0.44616714\n",
      "   0.32900002  0.31421752  0.47080035  0.46800631  0.32320767]]\n",
      "[[ 3.         32.          0.          6.         13.         14.\n",
      "  11.         20.         13.         14.         16.         16.\n",
      "   2.          0.32713542  0.47164586  0.42625319  0.40073023  0.39975219\n",
      "   0.42974446  0.45766948  0.32690128  0.32462625  0.44576905]]\n",
      "[[ 4.         16.          2.         15.         20.         13.\n",
      "  13.          4.          3.         16.          5.          6.\n",
      "   8.          0.36571535  0.49381274  0.47586912  0.36274272  0.46812956\n",
      "   0.3781344   0.49952414  0.33787918  0.32020123  0.33935999]]\n",
      "[[ 3.         64.          2.         17.         17.         14.\n",
      "  13.         12.         17.          7.         14.         10.\n",
      "   6.          0.42649834  0.49706214  0.43185342  0.43653382  0.4232126\n",
      "   0.38933675  0.4630812   0.46751654  0.32730653  0.33230261]]\n",
      "[[10.         32.          2.         12.          2.         20.\n",
      "   2.         10.         13.          3.         18.          7.\n",
      "  19.          0.33453927  0.40736471  0.38821839  0.45713367  0.47286075\n",
      "   0.40217869  0.39209576  0.48285134  0.35860754  0.33624305]]\n",
      "[[ 7.         16.          2.         14.          2.          4.\n",
      "   7.         11.          3.          5.          1.         18.\n",
      "  13.          0.44848693  0.45679922  0.39935808  0.33849774  0.30055514\n",
      "   0.37894873  0.39526587  0.38916961  0.49567693  0.48632533]]\n",
      "[[ 8.          8.          0.          1.          2.         20.\n",
      "  18.          9.          9.         18.         11.          5.\n",
      "   9.          0.37120921  0.45718586  0.36079721  0.36841959  0.31074811\n",
      "   0.48624708  0.44018585  0.35633598  0.47412643  0.39231013]]\n",
      "[[ 5.         64.          2.         17.         18.          4.\n",
      "  20.         10.         11.         18.          3.          8.\n",
      "  19.          0.38849887  0.41660853  0.35865617  0.37892413  0.43737845\n",
      "   0.35054215  0.40054423  0.44250377  0.47059513  0.38463532]]\n",
      "[[ 5.          8.          1.         14.          5.          9.\n",
      "   7.          1.          2.         17.         19.          7.\n",
      "  18.          0.49857452  0.36331582  0.30642792  0.41365022  0.4766648\n",
      "   0.35486177  0.38084462  0.35299932  0.35719046  0.39803213]]\n",
      "[[ 4.         16.          0.          2.          5.         19.\n",
      "   1.          3.          3.          3.          2.         19.\n",
      "   6.          0.43422992  0.33564193  0.44895263  0.40041426  0.36589085\n",
      "   0.41598876  0.440181    0.48286675  0.49500365  0.45278794]]\n",
      "[[ 5.         64.          2.         11.         14.         11.\n",
      "  19.         16.          2.         17.         18.         10.\n",
      "  14.          0.35207303  0.46163224  0.49216674  0.38863293  0.3044475\n",
      "   0.49596214  0.38062819  0.43835462  0.3402398   0.40048637]]\n",
      "[[ 2.         16.          1.         12.         19.          8.\n",
      "  14.         13.          9.          3.         20.         17.\n",
      "  15.          0.43326096  0.44334308  0.47946615  0.49266272  0.44032988\n",
      "   0.46352179  0.45404818  0.47225502  0.44689046  0.48286975]]\n",
      "[[ 1.          8.          1.         10.         18.          9.\n",
      "  19.         13.         16.          8.         14.         17.\n",
      "  14.          0.37493791  0.4971607   0.3556604   0.42546984  0.36896449\n",
      "   0.34254943  0.49855195  0.49613086  0.35373159  0.46807995]]\n",
      "[[ 9.         32.          2.         13.         17.          6.\n",
      "  16.          1.         17.          2.         14.         17.\n",
      "   5.          0.44230882  0.47107075  0.4699611   0.31768779  0.36997943\n",
      "   0.46590687  0.30872941  0.32516357  0.49356648  0.34330266]]\n",
      "[[10.         32.          1.          9.          5.          6.\n",
      "   6.          1.          2.          3.          3.          8.\n",
      "  16.          0.3583391   0.30591581  0.4721446   0.43398562  0.43471892\n",
      "   0.44321049  0.43783595  0.49957674  0.35383312  0.39173202]]\n",
      "[[ 7.         32.          0.         12.         10.         13.\n",
      "   9.          7.         12.         18.         17.         19.\n",
      "  12.          0.38808156  0.31325557  0.31659963  0.35734389  0.41167781\n",
      "   0.33628035  0.48972154  0.49255786  0.49214234  0.49034915]]\n",
      "[[ 3.         64.          2.         18.         20.         17.\n",
      "  18.         12.          5.         11.         13.         14.\n",
      "  11.          0.44038695  0.32245603  0.43529157  0.43221317  0.43439564\n",
      "   0.38446909  0.45784497  0.49777745  0.30880641  0.33157337]]\n",
      "[[ 6.         32.          1.          9.          7.         19.\n",
      "  12.         16.         18.         15.          2.         14.\n",
      "   5.          0.43593846  0.32212479  0.44815914  0.31918052  0.36462801\n",
      "   0.3555093   0.49660804  0.43848073  0.45580473  0.45276177]]\n",
      "[[ 7.         64.          2.         14.         11.          8.\n",
      "  11.         19.         19.         11.          7.          4.\n",
      "   3.          0.40362702  0.37619276  0.37637777  0.36333728  0.46991182\n",
      "   0.42829615  0.37194257  0.36883273  0.48588172  0.39542062]]\n",
      "[[ 8.         32.          0.          9.         17.         15.\n",
      "   9.          3.          2.         10.          9.          7.\n",
      "  18.          0.4113308   0.48537903  0.44052677  0.32876595  0.3855468\n",
      "   0.38578494  0.42287748  0.4436568   0.39880459  0.33689154]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.         32.          0.          1.          1.         11.\n",
      "   8.         20.         11.          5.         11.         16.\n",
      "  17.          0.31950726  0.48102604  0.44075471  0.35952177  0.38337483\n",
      "   0.40468365  0.49417307  0.33790268  0.4757302   0.49429023]]\n",
      "[[ 6.         16.          0.         17.         18.          5.\n",
      "  15.         13.         11.          8.         18.         18.\n",
      "  20.          0.43870324  0.40591472  0.49187831  0.44948995  0.403175\n",
      "   0.34152567  0.32521441  0.47810755  0.41515514  0.46412171]]\n",
      "[[ 1.         64.          1.         14.         15.          9.\n",
      "  18.          8.          4.          9.         13.         19.\n",
      "   6.          0.49276745  0.4720263   0.33487903  0.34741459  0.44854081\n",
      "   0.45733181  0.38716421  0.40037406  0.39720467  0.44099664]]\n",
      "[[ 5.         16.          1.         17.          9.         18.\n",
      "   7.         19.         20.          4.          6.         13.\n",
      "  13.          0.33366038  0.46769791  0.48841713  0.33858882  0.31438575\n",
      "   0.34963157  0.36478543  0.39912396  0.41643429  0.4481207 ]]\n",
      "[[ 9.         16.          0.         17.         15.          3.\n",
      "  16.         19.         15.          4.          9.         17.\n",
      "  15.          0.49882223  0.42363914  0.38271309  0.30548841  0.42926207\n",
      "   0.41681411  0.33941599  0.4079572   0.43490414  0.48962025]]\n",
      "[[ 1.         32.          0.         14.         15.         20.\n",
      "   1.          4.         11.         10.          1.         16.\n",
      "   8.          0.34616008  0.41844685  0.38093666  0.38768561  0.3606004\n",
      "   0.45693216  0.48303397  0.38416663  0.43450213  0.39394583]]\n",
      "[[ 4.         32.          0.          9.         20.         12.\n",
      "   8.          9.          6.          3.          5.         14.\n",
      "  18.          0.3065021   0.49489929  0.32239931  0.48508252  0.46202054\n",
      "   0.40260092  0.46244599  0.3917134   0.30657085  0.33840016]]\n",
      "[[ 4.         32.          1.         17.          3.          3.\n",
      "  19.          2.          8.          7.          1.          7.\n",
      "  10.          0.45519896  0.3121886   0.35626169  0.31767753  0.49755761\n",
      "   0.45491395  0.37319395  0.3919313   0.43287952  0.31348008]]\n",
      "[[ 7.         64.          2.          1.          7.         17.\n",
      "  16.         12.         18.          6.         13.          6.\n",
      "  18.          0.39519768  0.30749544  0.41512117  0.37151546  0.46902898\n",
      "   0.38547205  0.35561301  0.49012176  0.371538    0.30835712]]\n",
      "[[ 1.         64.          2.          3.         20.         12.\n",
      "   1.          9.          2.          4.          3.         12.\n",
      "  13.          0.47392743  0.31740205  0.37744427  0.39126616  0.43543078\n",
      "   0.43150673  0.48257622  0.36965137  0.4542792   0.42083755]]\n",
      "[[ 8.         16.          0.         18.          6.          8.\n",
      "   5.         15.         11.         13.         11.          5.\n",
      "   7.          0.31197706  0.46726176  0.41856337  0.36983421  0.48608565\n",
      "   0.49517829  0.34782114  0.40375874  0.33799779  0.44726458]]\n",
      "[[ 4.          8.          0.         19.          6.         14.\n",
      "   5.         18.          4.          9.         20.         14.\n",
      "  11.          0.39262956  0.3993749   0.31919469  0.36526684  0.3026198\n",
      "   0.39179812  0.42529704  0.32824413  0.31262863  0.44046129]]\n",
      "[[10.          8.          1.         13.         15.          6.\n",
      "  14.         18.         12.          4.         14.         14.\n",
      "  16.          0.46979123  0.31172485  0.45742123  0.37284193  0.44584164\n",
      "   0.32167361  0.41242761  0.36728042  0.33463552  0.34567539]]\n",
      "[[ 8.         16.          0.         18.         17.         16.\n",
      "  15.         16.          1.          1.         11.         19.\n",
      "  19.          0.45545007  0.38357591  0.4580744   0.4839963   0.36652798\n",
      "   0.40412893  0.34135627  0.45303535  0.38508097  0.47192307]]\n",
      "[[ 6.         16.          2.          9.         20.         14.\n",
      "  18.         17.          8.         12.          3.          8.\n",
      "  20.          0.37025197  0.48909787  0.39685057  0.32450964  0.38029878\n",
      "   0.40149391  0.33335646  0.41220677  0.35474762  0.38188213]]\n",
      "[[ 4.         32.          0.          8.          3.          7.\n",
      "   9.          7.         19.          7.         18.          5.\n",
      "  20.          0.48564134  0.30742026  0.40390486  0.47719361  0.46827797\n",
      "   0.38454404  0.41765828  0.4253892   0.35574125  0.33711009]]\n",
      "[[ 8.          8.          2.         11.         15.         12.\n",
      "   9.         14.         19.          9.          9.         12.\n",
      "  17.          0.47343151  0.39447111  0.48388159  0.41153776  0.35662794\n",
      "   0.3492399   0.40468676  0.30452855  0.32186541  0.38925084]]\n",
      "[[ 6.         64.          2.          4.          6.         15.\n",
      "   3.         10.          9.         20.          2.         11.\n",
      "   5.          0.45215404  0.35910006  0.47888845  0.35284484  0.35694408\n",
      "   0.31131599  0.49653971  0.3259212   0.40575384  0.40186556]]\n",
      "[[ 2.         32.          1.         11.          8.          4.\n",
      "  19.         14.         20.          2.         20.          8.\n",
      "  16.          0.37471629  0.39287865  0.45658043  0.4368666   0.49579466\n",
      "   0.33675892  0.43156596  0.35664136  0.34381913  0.32958702]]\n",
      "[[8.         8.         2.         4.         2.         2.\n",
      "  8.         7.         2.         4.         2.         9.\n",
      "  3.         0.31271246 0.35517506 0.40956345 0.48444001 0.36180493\n",
      "  0.43937569 0.308727   0.32679645 0.3138465  0.45658461]]\n",
      "[[10.         32.          1.         14.          5.         18.\n",
      "   5.         15.         18.          4.          7.          1.\n",
      "   6.          0.34832412  0.48249876  0.49454309  0.39241593  0.3173378\n",
      "   0.39346458  0.38612217  0.49988266  0.49833782  0.40694842]]\n",
      "[[ 9.         16.          0.          7.          3.         18.\n",
      "  15.          3.         19.         19.         20.         19.\n",
      "  10.          0.42172884  0.45040087  0.46568272  0.33376274  0.44673145\n",
      "   0.31986436  0.44796227  0.35885806  0.37221338  0.36131042]]\n",
      "[[ 6.         64.          0.          5.          1.         11.\n",
      "  12.          2.         14.         18.         12.         14.\n",
      "   2.          0.45088629  0.30219331  0.43821717  0.38453311  0.47551704\n",
      "   0.49578418  0.47646516  0.31247143  0.35925889  0.42296763]]\n",
      "[[ 2.         64.          0.         20.         18.         12.\n",
      "  10.          5.          9.          7.          8.         14.\n",
      "   6.          0.38664118  0.42693496  0.39117484  0.36340771  0.42086714\n",
      "   0.46552366  0.33903685  0.45668545  0.45237202  0.39304516]]\n",
      "[[ 8.          8.          1.         18.         19.         19.\n",
      "   6.         10.          2.          1.          4.          5.\n",
      "  12.          0.43348526  0.37324503  0.40754842  0.43638474  0.40446383\n",
      "   0.32875206  0.35059694  0.4920868   0.33416442  0.36738969]]\n",
      "[[ 8.         64.          2.          5.         12.         19.\n",
      "  13.          6.          7.          6.         11.         11.\n",
      "  19.          0.44817304  0.37534716  0.49940636  0.39219971  0.49703902\n",
      "   0.41795895  0.4775974   0.32438839  0.34338814  0.49205055]]\n",
      "[[10.          8.          2.          4.         13.         19.\n",
      "   6.          9.          1.          3.         19.          7.\n",
      "  13.          0.37466954  0.3568206   0.38796209  0.3403241   0.3870663\n",
      "   0.32097012  0.33082798  0.47718152  0.47172055  0.32830489]]\n",
      "[[ 2.         32.          0.          9.         15.          4.\n",
      "  10.          7.          4.          8.          7.          2.\n",
      "  13.          0.43627078  0.44854954  0.4807362   0.36310942  0.46715729\n",
      "   0.30035258  0.4527232   0.43305948  0.36710915  0.3821288 ]]\n",
      "[[ 1.         64.          1.          6.         14.         20.\n",
      "   3.         16.         19.         20.          9.         17.\n",
      "  20.          0.37496814  0.47540761  0.49742745  0.47767816  0.38106963\n",
      "   0.47247698  0.35049735  0.33186636  0.4574665   0.33476756]]\n",
      "[[ 7.         64.          2.         10.          9.         17.\n",
      "   8.         17.         15.          2.          3.          6.\n",
      "   8.          0.42069423  0.38355937  0.35641239  0.49430173  0.37362263\n",
      "   0.40064898  0.38197686  0.37640232  0.39730388  0.39801552]]\n",
      "[[ 6.         64.          1.          5.          8.          9.\n",
      "  19.          3.         11.          4.          8.         16.\n",
      "  18.          0.49949396  0.49529591  0.48433696  0.45677065  0.32651531\n",
      "   0.42225668  0.47532736  0.33941034  0.40888056  0.48029179]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          8.          1.          8.          8.          1.\n",
      "  17.          3.         17.         16.         12.         14.\n",
      "  11.          0.37042979  0.39547868  0.34830204  0.4364368   0.4600835\n",
      "   0.49960011  0.48140952  0.32480261  0.35420315  0.37123211]]\n",
      "[[ 8.         64.          0.         10.          1.          1.\n",
      "  12.          4.          1.         10.          6.         12.\n",
      "   4.          0.45117267  0.43582292  0.49495964  0.4001037   0.38727777\n",
      "   0.47443988  0.40442428  0.49767732  0.41383096  0.31475171]]\n",
      "[[ 8.         32.          2.         11.          2.         12.\n",
      "   3.         12.         16.         17.         14.         18.\n",
      "   1.          0.48207582  0.42362533  0.46129457  0.41036657  0.40772816\n",
      "   0.44153633  0.36067047  0.47951722  0.43480124  0.40058464]]\n",
      "[[ 6.         16.          1.         16.          6.         11.\n",
      "   4.          3.          7.          9.         15.         19.\n",
      "   5.          0.32107676  0.44602667  0.38333628  0.36600292  0.32877298\n",
      "   0.34561016  0.44924627  0.46613297  0.35184316  0.45399214]]\n",
      "[[ 7.         32.          2.         12.         17.         20.\n",
      "  10.         20.          3.          2.          5.          4.\n",
      "   5.          0.37597     0.37827075  0.37241087  0.40976439  0.43018463\n",
      "   0.39550355  0.42452552  0.34448248  0.40447529  0.33034057]]\n",
      "[[ 3.         16.          1.          3.         15.         13.\n",
      "  16.         20.          6.          2.          8.          4.\n",
      "  18.          0.42534503  0.39633329  0.38947784  0.37859409  0.49585718\n",
      "   0.47067547  0.33510035  0.49568615  0.42495204  0.32875177]]\n",
      "[[ 9.          8.          2.         18.         15.          4.\n",
      "  19.         14.         17.         11.         11.          7.\n",
      "   6.          0.45848697  0.30125881  0.332085    0.41565681  0.41544848\n",
      "   0.30289439  0.42069712  0.43563971  0.36174638  0.38967306]]\n",
      "[[10.         32.          1.          7.          1.         15.\n",
      "  20.         13.          4.         18.         16.          6.\n",
      "   2.          0.44261453  0.41489473  0.46408403  0.4028693   0.35065547\n",
      "   0.37545686  0.3215689   0.43514781  0.44486514  0.45381428]]\n",
      "[[ 3.         16.          0.          8.          7.          4.\n",
      "   3.          9.         14.          5.          8.          1.\n",
      "  11.          0.39841335  0.38701385  0.47311977  0.49353601  0.32469424\n",
      "   0.47805055  0.37156143  0.42629718  0.48355019  0.39887054]]\n",
      "[[ 5.         64.          2.         20.         11.         17.\n",
      "  12.          6.          2.         16.          3.         14.\n",
      "   4.          0.36606122  0.37675148  0.33040654  0.45991309  0.45736478\n",
      "   0.3037295   0.38607843  0.4400845   0.30683662  0.35531603]]\n",
      "[[ 3.         32.          1.         17.          3.         13.\n",
      "  19.         14.         16.         18.          1.         16.\n",
      "   4.          0.31935145  0.45835486  0.48920339  0.4591617   0.47448428\n",
      "   0.47582733  0.34345529  0.4593287   0.48304066  0.38711113]]\n",
      "[[10.          8.          1.         15.         17.          1.\n",
      "   6.         12.         17.         10.          2.          7.\n",
      "  11.          0.35651861  0.32810669  0.38174625  0.39919655  0.43072126\n",
      "   0.37334913  0.30872942  0.40834305  0.38702473  0.30675292]]\n",
      "[[ 8.         32.          1.         18.         15.         16.\n",
      "  16.         11.          6.          6.         13.         15.\n",
      "  15.          0.43639372  0.45498442  0.3240976   0.32567781  0.33098354\n",
      "   0.31215734  0.48287149  0.49953305  0.32864896  0.44367313]]\n",
      "[[ 3.          8.          1.         13.         20.         10.\n",
      "  17.          6.          6.          9.         12.         14.\n",
      "   8.          0.35980589  0.37887506  0.441844    0.39101373  0.3709291\n",
      "   0.42174786  0.33457745  0.41494518  0.34631707  0.41105712]]\n",
      "[[ 1.          8.          2.         17.         18.          1.\n",
      "  14.         11.          5.         13.         12.          6.\n",
      "  18.          0.34719312  0.44431533  0.49133024  0.31235762  0.48312898\n",
      "   0.41925121  0.31430516  0.37929972  0.39890976  0.30872998]]\n",
      "[[ 3.         32.          1.          1.         15.         11.\n",
      "  16.         11.          5.          3.          9.         19.\n",
      "  13.          0.40251064  0.35840565  0.48317949  0.34067749  0.4100915\n",
      "   0.44487241  0.49789003  0.43236343  0.4588553   0.38066793]]\n",
      "[[ 7.         32.          1.          9.          5.         17.\n",
      "   6.         15.         17.          4.          2.         12.\n",
      "  20.          0.47915473  0.42446218  0.36476162  0.3270718   0.44999166\n",
      "   0.30788075  0.47528523  0.4594535   0.36403018  0.39700179]]\n",
      "[[ 4.         64.          0.          7.         20.          7.\n",
      "  18.          5.         15.         13.          7.         13.\n",
      "  12.          0.46020673  0.35376465  0.49667399  0.33979666  0.42278494\n",
      "   0.36828272  0.49426658  0.32839834  0.33288292  0.42041198]]\n",
      "[[ 1.         64.          0.         13.          8.          8.\n",
      "  11.         15.          3.          9.          2.          3.\n",
      "   5.          0.46547846  0.3929367   0.37033655  0.31180728  0.36368236\n",
      "   0.46069996  0.49669994  0.45987122  0.33424437  0.40982192]]\n",
      "[[10.          8.          0.         11.         10.         14.\n",
      "  13.          9.         17.         18.         20.          1.\n",
      "   1.          0.48686912  0.41997042  0.30744112  0.3280749   0.36258227\n",
      "   0.47419853  0.34582785  0.38762777  0.40844797  0.47232652]]\n",
      "[[ 7.          8.          1.          1.         17.          7.\n",
      "  16.          6.          6.         12.         15.         13.\n",
      "   4.          0.45199781  0.35232861  0.49750763  0.491946    0.31395819\n",
      "   0.40993774  0.38997655  0.32768777  0.42318525  0.45315675]]\n",
      "[[ 4.          8.          2.         15.          7.         12.\n",
      "   2.         12.          1.          7.         12.         18.\n",
      "  19.          0.34681281  0.36197056  0.37133702  0.47464521  0.39620331\n",
      "   0.30768057  0.42875283  0.46605178  0.30617654  0.48936248]]\n",
      "[[ 5.         32.          0.         13.         15.         16.\n",
      "   9.          4.         17.          6.         10.         19.\n",
      "  17.          0.49313736  0.36797045  0.49741025  0.46389807  0.34461731\n",
      "   0.41885969  0.31546002  0.36442976  0.45801336  0.39748826]]\n",
      "[[ 9.         64.          0.         12.         12.         10.\n",
      "  17.         18.         14.         19.          2.         18.\n",
      "   1.          0.31337243  0.3035721   0.45754779  0.40490891  0.36279633\n",
      "   0.47889971  0.49495332  0.42465202  0.43087432  0.33752905]]\n",
      "[[ 9.         16.          0.          3.          8.         15.\n",
      "  11.         17.         17.          6.         17.          1.\n",
      "   6.          0.43392646  0.45057502  0.33654567  0.4744277   0.42274798\n",
      "   0.49050528  0.4706554   0.40682907  0.43930218  0.41042036]]\n",
      "[[ 5.         32.          0.          9.          6.         15.\n",
      "  11.          2.         10.          4.         15.          6.\n",
      "   7.          0.47680679  0.33236763  0.30901483  0.43193478  0.41895177\n",
      "   0.31987752  0.4927266   0.37248262  0.44876866  0.35370179]]\n",
      "[[10.         64.          2.         15.         13.         16.\n",
      "  16.         16.          9.         18.         18.          7.\n",
      "   3.          0.41358466  0.44532254  0.46561832  0.40001073  0.41373658\n",
      "   0.48153859  0.39123148  0.39639215  0.45746179  0.40088486]]\n",
      "[[ 7.         32.          0.          1.         14.          3.\n",
      "   6.         12.          8.          4.         15.          1.\n",
      "   8.          0.38328484  0.37363196  0.4492947   0.32878023  0.30925515\n",
      "   0.35199642  0.48371633  0.42107494  0.46947237  0.40407615]]\n",
      "[[ 4.         32.          2.         11.         18.         16.\n",
      "  15.         18.         10.         13.         14.         20.\n",
      "   3.          0.41369709  0.40659224  0.46393323  0.4753903   0.45537399\n",
      "   0.49721378  0.49105818  0.37686454  0.4286537   0.47792471]]\n",
      "[[ 5.         16.          2.          2.          6.          5.\n",
      "   9.          5.          9.          1.         13.         13.\n",
      "  15.          0.39033795  0.49339034  0.30776752  0.49146388  0.33028102\n",
      "   0.482653    0.39705076  0.35743455  0.39875333  0.33831522]]\n",
      "[[ 1.         16.          2.          1.         17.          7.\n",
      "  12.          4.          8.          4.          9.         20.\n",
      "   4.          0.35202767  0.35652867  0.33761461  0.34904078  0.31607875\n",
      "   0.44200319  0.32037494  0.40858175  0.43429145  0.41952048]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.         64.          1.         10.         20.         20.\n",
      "   3.         12.          9.          2.         13.         16.\n",
      "   8.          0.31701925  0.40546975  0.41374062  0.39191928  0.35064109\n",
      "   0.35759407  0.44787623  0.39095702  0.35278022  0.37904823]]\n",
      "[[ 2.         64.          1.         18.         10.         14.\n",
      "   7.         15.          5.         13.          8.         16.\n",
      "   9.          0.3801545   0.3481501   0.48100508  0.44866042  0.43956821\n",
      "   0.38467363  0.36888578  0.41139367  0.33121761  0.41338716]]\n",
      "[[ 6.          8.          2.         20.         18.          7.\n",
      "  12.          2.         17.         11.         16.          2.\n",
      "   8.          0.3200751   0.39301699  0.31574815  0.37140613  0.39241529\n",
      "   0.48784993  0.35090705  0.47950397  0.4510796   0.36815311]]\n",
      "[[ 8.         16.          0.          6.         19.         16.\n",
      "  12.          6.          5.         16.          6.          3.\n",
      "  10.          0.30244805  0.47174704  0.4489901   0.35132476  0.4740128\n",
      "   0.31034168  0.30546565  0.46625984  0.41863076  0.45571064]]\n",
      "[[ 2.         16.          2.          4.          3.          4.\n",
      "  12.          7.         13.          9.         14.         18.\n",
      "   1.          0.31876267  0.4009867   0.32849837  0.36516141  0.49051497\n",
      "   0.34356025  0.36308866  0.43926286  0.45812975  0.49982894]]\n",
      "[[ 4.         16.          2.         15.         13.          1.\n",
      "   7.         19.          8.          1.         17.         14.\n",
      "  15.          0.33010855  0.34823132  0.33848851  0.35466063  0.39856672\n",
      "   0.31580315  0.44682854  0.3256947   0.45110216  0.42599134]]\n",
      "[[ 2.          8.          0.          6.          3.         18.\n",
      "   9.          2.          6.         12.         10.         13.\n",
      "   7.          0.46161259  0.34619048  0.35338614  0.31339855  0.48239997\n",
      "   0.42323481  0.46472326  0.38821699  0.44054534  0.48542631]]\n",
      "[[ 8.         32.          0.         20.          9.         19.\n",
      "   2.         11.          4.          9.         12.         19.\n",
      "   6.          0.44265934  0.34156953  0.47598373  0.38881195  0.34941315\n",
      "   0.36154693  0.47690153  0.4668218   0.48349094  0.39089867]]\n",
      "[[10.         32.          1.         18.         10.         16.\n",
      "  15.         10.         20.          1.         11.          4.\n",
      "   2.          0.33275805  0.30296736  0.40384392  0.49709131  0.30666848\n",
      "   0.49233229  0.37359987  0.36865685  0.37796955  0.3825667 ]]\n",
      "[[ 8.         32.          1.          9.          5.          2.\n",
      "  10.         10.          4.         11.          8.         14.\n",
      "   7.          0.32887402  0.39450563  0.31131747  0.49659098  0.32511972\n",
      "   0.37640818  0.38210643  0.33986541  0.40329937  0.31541231]]\n",
      "[[ 7.          8.          1.          3.          5.         18.\n",
      "  11.          1.         15.          3.         15.          6.\n",
      "   5.          0.35135605  0.33892165  0.42946069  0.31301649  0.31660333\n",
      "   0.37056586  0.39354349  0.4966517   0.45241318  0.44924445]]\n",
      "[[ 1.         16.          0.         14.         18.         10.\n",
      "  17.          3.         16.          5.         11.         12.\n",
      "   7.          0.36292756  0.49946692  0.40357372  0.47121387  0.45999843\n",
      "   0.33100176  0.47729704  0.31810425  0.30503933  0.32100744]]\n",
      "[[ 2.         32.          1.         17.          3.         18.\n",
      "   5.         18.         15.         18.         18.         13.\n",
      "  11.          0.4997054   0.31039677  0.48549028  0.33497648  0.46666414\n",
      "   0.44763364  0.41342249  0.44900616  0.42302785  0.31438637]]\n",
      "[[ 2.         16.          0.         16.          6.          6.\n",
      "   9.          5.         18.          9.          9.          7.\n",
      "   4.          0.44509815  0.39704955  0.46982154  0.37907506  0.4171574\n",
      "   0.40619608  0.35477979  0.43630429  0.4470048   0.36420958]]\n",
      "[[10.         64.          0.         12.         11.          1.\n",
      "   6.          6.          7.          1.          4.         10.\n",
      "   5.          0.47661105  0.42239016  0.33870958  0.38032719  0.47799663\n",
      "   0.46508642  0.38461269  0.38602602  0.44609732  0.31917038]]\n",
      "[[ 7.         64.          0.         13.         11.         18.\n",
      "   1.         20.         20.          3.         18.         13.\n",
      "   1.          0.4307854   0.31733519  0.36231808  0.4761167   0.4964214\n",
      "   0.32286478  0.40037112  0.49251512  0.47396254  0.38510757]]\n",
      "[[ 8.          8.          2.          3.         15.          3.\n",
      "  16.          5.          2.          6.          7.         16.\n",
      "   7.          0.34941811  0.43033206  0.48772368  0.35264463  0.32444594\n",
      "   0.30829781  0.43721303  0.44626414  0.46877849  0.42619479]]\n",
      "[[10.         32.          1.         17.          5.         20.\n",
      "   4.         13.         19.         15.         18.         17.\n",
      "   2.          0.31200052  0.3101801   0.41690384  0.39898416  0.42080408\n",
      "   0.46093575  0.46216867  0.33367356  0.3077241   0.46902602]]\n",
      "[[ 4.         32.          2.         16.          4.          2.\n",
      "  19.          7.         18.          8.          8.         16.\n",
      "   4.          0.4212571   0.45426119  0.48320748  0.31314747  0.38234358\n",
      "   0.38640439  0.4171588   0.33077706  0.33667397  0.3598175 ]]\n",
      "[[ 2.         64.          2.         13.          4.          5.\n",
      "   7.          7.         13.         10.          5.          9.\n",
      "  13.          0.31538181  0.46712647  0.33738027  0.30903179  0.45964622\n",
      "   0.42700097  0.47274124  0.46122786  0.34874162  0.35258282]]\n",
      "[[ 5.         32.          1.         19.          3.         16.\n",
      "  17.          4.         19.         11.          3.          5.\n",
      "   7.          0.40615066  0.36229892  0.37888365  0.46822704  0.42199507\n",
      "   0.31100693  0.42069177  0.46327121  0.42553427  0.40563194]]\n",
      "[[10.         64.          0.          2.         15.         18.\n",
      "   3.          4.         11.          6.          8.         15.\n",
      "   2.          0.33863414  0.48172735  0.48075591  0.37645616  0.30682532\n",
      "   0.30023374  0.35924424  0.4948428   0.31117     0.42820271]]\n",
      "[[ 2.          8.          1.          8.          8.         13.\n",
      "  11.         18.          7.         11.          3.         13.\n",
      "   2.          0.33988557  0.37476073  0.38278142  0.43432658  0.32084692\n",
      "   0.49389381  0.35674896  0.30626401  0.46581416  0.39382446]]\n",
      "[[ 9.         32.          1.          2.          1.         11.\n",
      "  19.          3.          3.         15.         11.         10.\n",
      "   8.          0.3875718   0.48095696  0.30747824  0.31766897  0.33388531\n",
      "   0.39974919  0.30253448  0.38659607  0.36762161  0.47998816]]\n",
      "[[10.          8.          0.          5.          7.          5.\n",
      "   8.         17.         13.          5.         15.         14.\n",
      "  11.          0.35084588  0.49643641  0.45786406  0.38779406  0.4221965\n",
      "   0.40156913  0.45003032  0.35952797  0.30878823  0.45029264]]\n",
      "[[10.          8.          1.          2.         19.         10.\n",
      "   6.         10.          3.         10.          5.          1.\n",
      "   3.          0.41402607  0.44773971  0.3932068   0.46177772  0.43171878\n",
      "   0.36907888  0.44913621  0.38970824  0.34667957  0.39209147]]\n",
      "[[10.          8.          0.         20.          4.          8.\n",
      "  18.         19.          4.         15.          6.         10.\n",
      "  20.          0.41372735  0.30490753  0.44804905  0.39220717  0.47426829\n",
      "   0.49618933  0.35181385  0.41606962  0.47908614  0.39584711]]\n",
      "[[ 7.         64.          2.         12.         10.          3.\n",
      "  13.         15.          9.          4.         16.         17.\n",
      "  10.          0.30736481  0.3192397   0.35970076  0.4402633   0.31151303\n",
      "   0.34571342  0.42627205  0.41309323  0.4711752   0.37726406]]\n",
      "[[10.         32.          0.         10.         12.         15.\n",
      "  18.         12.          4.          1.         20.          4.\n",
      "   2.          0.39306024  0.35055212  0.36031939  0.32786592  0.41944354\n",
      "   0.49626569  0.41938892  0.33652573  0.35816998  0.4558991 ]]\n",
      "[[ 2.          8.          2.         18.          8.         14.\n",
      "  17.         11.          2.          9.          3.         10.\n",
      "  14.          0.47853356  0.41885589  0.45960453  0.47270303  0.47195479\n",
      "   0.32306787  0.35398827  0.42914731  0.35525023  0.41610941]]\n",
      "[[ 3.         64.          0.          7.         11.         15.\n",
      "  11.          5.         19.          8.         18.         19.\n",
      "   7.          0.47577653  0.46710497  0.34688774  0.34291111  0.31990729\n",
      "   0.33791854  0.49169644  0.49601465  0.35236125  0.42205875]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          8.          0.         18.         10.         10.\n",
      "   1.         20.         16.         17.         19.         13.\n",
      "  16.          0.30668159  0.30896022  0.37474547  0.4780972   0.41134563\n",
      "   0.45104064  0.30391171  0.48602654  0.32521619  0.47332677]]\n",
      "[[ 3.         32.          2.         14.          1.         10.\n",
      "  18.         18.          6.         20.         18.         10.\n",
      "  15.          0.37949932  0.4885714   0.34745821  0.4390041   0.33415655\n",
      "   0.43927343  0.45979252  0.36344599  0.39616703  0.41692391]]\n",
      "[[ 1.          8.          1.          8.         16.          3.\n",
      "   3.         11.          9.          8.          3.         14.\n",
      "   5.          0.47377378  0.40735177  0.32177867  0.41765107  0.4166536\n",
      "   0.44698444  0.30698033  0.47517748  0.43155299  0.35277197]]\n",
      "[[ 5.         64.          0.          5.          2.         15.\n",
      "   3.         15.          8.         16.          7.          7.\n",
      "  11.          0.42425228  0.30227356  0.33023775  0.30955204  0.42095361\n",
      "   0.32584692  0.47003799  0.3386507   0.45332911  0.41849011]]\n",
      "[[10.         32.          0.         18.         15.          6.\n",
      "  15.          2.         10.          6.          1.          7.\n",
      "  17.          0.35573137  0.31225199  0.43701055  0.47189027  0.48076379\n",
      "   0.45253709  0.38259163  0.4638598   0.33797204  0.33255153]]\n",
      "[[ 8.          8.          1.         15.         11.         17.\n",
      "  19.          7.         17.          6.         18.         20.\n",
      "   5.          0.36796221  0.39046638  0.47862627  0.39439008  0.42469993\n",
      "   0.39345783  0.46773643  0.49462512  0.47708813  0.41460197]]\n",
      "[[ 4.         32.          1.          1.          3.         10.\n",
      "   2.          8.         10.         19.         20.          1.\n",
      "  14.          0.33857948  0.42419556  0.45938374  0.335835    0.33903965\n",
      "   0.45715288  0.36276865  0.48343424  0.44206121  0.38810335]]\n",
      "[[10.         16.          1.         17.         15.         10.\n",
      "   6.         17.          6.          6.         19.          6.\n",
      "  12.          0.34123961  0.30613325  0.38445384  0.3357172   0.48142619\n",
      "   0.45488024  0.39627513  0.45267887  0.48645695  0.31686779]]\n",
      "[[ 1.         64.          1.          5.         18.          5.\n",
      "  13.         18.          7.         14.         14.         13.\n",
      "   3.          0.48473306  0.35403254  0.42328933  0.37394434  0.37845219\n",
      "   0.37184139  0.32858617  0.46171771  0.44184663  0.41269155]]\n",
      "[[ 6.         16.          2.          4.         20.         13.\n",
      "   4.          8.         16.          1.         17.          6.\n",
      "   4.          0.44969106  0.37262071  0.36643821  0.46184934  0.3934985\n",
      "   0.32182182  0.4857508   0.37411135  0.4104233   0.49971598]]\n",
      "[[ 7.          8.          2.         19.         10.          1.\n",
      "   2.          5.          4.          9.          7.          9.\n",
      "   7.          0.34126466  0.44071546  0.32877442  0.30650319  0.32591294\n",
      "   0.31699624  0.4388752   0.46224378  0.33807251  0.31399329]]\n",
      "[[10.         16.          0.          5.          6.          4.\n",
      "  19.         14.          6.          5.         17.         19.\n",
      "  13.          0.40608355  0.48721787  0.44024991  0.37885686  0.30869455\n",
      "   0.41288209  0.49928317  0.47164273  0.4701939   0.4058475 ]]\n",
      "[[ 8.         16.          1.          1.         14.         16.\n",
      "  13.          5.         18.          1.          1.         13.\n",
      "   4.          0.35996045  0.39375614  0.39881248  0.38770464  0.31493093\n",
      "   0.36787406  0.3379995   0.48948198  0.39700389  0.4157093 ]]\n",
      "[[ 9.         64.          1.          8.          4.          9.\n",
      "  20.         20.         17.         15.          8.          3.\n",
      "   5.          0.32221892  0.42789151  0.46972059  0.47922019  0.40165609\n",
      "   0.35574567  0.48747979  0.49193656  0.40478657  0.49350617]]\n",
      "[[ 2.         32.          0.          7.          8.          6.\n",
      "   3.          4.         18.          1.          7.         12.\n",
      "  14.          0.3594139   0.39295691  0.36569617  0.30153782  0.42444092\n",
      "   0.48216004  0.4052524   0.41929577  0.4810781   0.34613554]]\n",
      "[[ 2.         32.          2.          8.          7.          9.\n",
      "  15.         20.          2.         10.         15.         13.\n",
      "   6.          0.33551476  0.38159017  0.37764196  0.47706516  0.34465999\n",
      "   0.40826546  0.47273258  0.4220198   0.37496283  0.4894032 ]]\n",
      "[[ 4.         16.          1.          1.          2.         14.\n",
      "  15.          3.         11.         20.          6.         14.\n",
      "  15.          0.42992584  0.31530248  0.35293305  0.49748332  0.42395314\n",
      "   0.42763144  0.44585482  0.45609318  0.44352851  0.37481359]]\n",
      "[[10.         16.          0.         20.         20.         15.\n",
      "  15.         14.          6.          4.         20.          5.\n",
      "  14.          0.32417124  0.48385402  0.42716891  0.4237906   0.34459671\n",
      "   0.43511651  0.48439345  0.42678192  0.38624459  0.41036286]]\n",
      "[[ 4.         64.          2.         20.          2.          2.\n",
      "  11.         14.          5.         15.          5.          6.\n",
      "  11.          0.39193834  0.33034324  0.45284897  0.49025204  0.38228498\n",
      "   0.4780047   0.39446006  0.40209741  0.49296541  0.40047888]]\n",
      "[[ 8.         32.          0.         19.         17.         16.\n",
      "  20.         15.          6.         15.         18.          4.\n",
      "   4.          0.324114    0.35880148  0.41751319  0.38894955  0.34372698\n",
      "   0.4166543   0.49865872  0.33528425  0.4194133   0.42713024]]\n",
      "[[ 3.         64.          2.         16.          1.          8.\n",
      "   7.          3.          7.         18.         17.          1.\n",
      "   8.          0.39144987  0.44041281  0.40402056  0.40165558  0.38507605\n",
      "   0.43147965  0.45063624  0.33634096  0.37008994  0.47007964]]\n",
      "[[ 7.         64.          1.         20.         17.         13.\n",
      "   4.         14.          9.          5.          8.          8.\n",
      "  15.          0.42495075  0.3577838   0.48637968  0.37198068  0.48351396\n",
      "   0.45646512  0.32823118  0.41056001  0.41174714  0.30620161]]\n",
      "[[ 8.          8.          2.         17.         15.          4.\n",
      "  17.         19.          9.         18.          4.          4.\n",
      "   1.          0.37403719  0.31408272  0.45832349  0.4029047   0.36510226\n",
      "   0.37870063  0.33068875  0.30697367  0.36658058  0.42661462]]\n",
      "[[10.         32.          0.          8.         13.         16.\n",
      "   5.         13.         13.         20.          7.         12.\n",
      "   7.          0.43407401  0.35381433  0.46342968  0.34195832  0.44971861\n",
      "   0.47352148  0.33492457  0.35892017  0.45106954  0.49164554]]\n",
      "[[ 5.         64.          0.          3.         13.          8.\n",
      "   7.         12.          9.          8.         14.         14.\n",
      "  17.          0.36996029  0.30561332  0.49876523  0.32253839  0.39786355\n",
      "   0.30737143  0.40923486  0.33136165  0.47600501  0.43196325]]\n",
      "[[ 3.          8.          0.          8.         16.          7.\n",
      "  20.         12.         18.          1.          8.         17.\n",
      "  20.          0.39653823  0.36791211  0.31226108  0.37079831  0.47728513\n",
      "   0.39800058  0.45403379  0.37547986  0.37608199  0.48105191]]\n",
      "[[ 4.         32.          1.         20.          5.          2.\n",
      "  18.         15.         17.          7.          8.         11.\n",
      "   3.          0.45394844  0.34218335  0.4774532   0.38905989  0.40654017\n",
      "   0.37751423  0.30306698  0.45179015  0.31697917  0.45575957]]\n",
      "[[ 9.         32.          0.         16.         20.         10.\n",
      "  17.          8.         15.         17.          6.         15.\n",
      "   8.          0.41321619  0.49908776  0.35709928  0.40228215  0.3037884\n",
      "   0.49674609  0.3998011   0.35907696  0.48428634  0.38850174]]\n",
      "[[ 6.         64.          1.          8.         10.         11.\n",
      "  11.          2.          4.          1.         16.         19.\n",
      "  16.          0.34703839  0.34520169  0.32830096  0.4165186   0.44972445\n",
      "   0.38781901  0.37309247  0.48705904  0.41303102  0.48151418]]\n",
      "[[ 9.         16.          2.          5.         13.          4.\n",
      "  19.          6.         17.          3.          1.          6.\n",
      "   9.          0.37038124  0.41896542  0.37116944  0.48454369  0.44969164\n",
      "   0.3930681   0.41586612  0.36496641  0.47450064  0.33753234]]\n",
      "[[ 9.         64.          0.          1.         12.          4.\n",
      "   7.          4.          9.         14.         10.          2.\n",
      "   4.          0.37504113  0.3792733   0.41267284  0.47438008  0.48837138\n",
      "   0.32252122  0.32363327  0.48356303  0.40922569  0.30191322]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.         32.          1.          8.         16.          5.\n",
      "   8.          7.          1.          8.         11.          6.\n",
      "  17.          0.49457644  0.42634349  0.34355385  0.32750477  0.33769752\n",
      "   0.45733197  0.36497217  0.30292545  0.30881128  0.30701157]]\n",
      "[[ 4.         16.          1.         19.          4.         15.\n",
      "  16.          2.          1.         20.         14.         17.\n",
      "  13.          0.49837855  0.33500207  0.40988081  0.45339225  0.35843882\n",
      "   0.45862349  0.41693677  0.45883271  0.33819584  0.30235886]]\n",
      "[[ 4.          8.          0.          7.         20.         15.\n",
      "  19.         18.         14.          9.         11.         19.\n",
      "  18.          0.30411055  0.48628203  0.42100914  0.43113101  0.42512364\n",
      "   0.48404284  0.46321957  0.37966136  0.48629693  0.41904844]]\n",
      "[[ 2.         64.          2.         16.         11.         10.\n",
      "   6.          9.         18.          3.         10.          6.\n",
      "  17.          0.4199945   0.48267524  0.48202242  0.33921389  0.39033227\n",
      "   0.31653994  0.44811882  0.41617231  0.39844501  0.41861432]]\n",
      "[[10.         16.          1.          1.         18.         20.\n",
      "   2.         11.         14.         10.         14.         15.\n",
      "  13.          0.48042995  0.43933381  0.36996867  0.34921993  0.40356253\n",
      "   0.48290162  0.3109924   0.43299546  0.38311111  0.34375454]]\n",
      "[[ 1.         16.          1.         17.         18.          3.\n",
      "   1.         12.          5.         10.         16.         14.\n",
      "  13.          0.49179165  0.44517665  0.40502781  0.34017282  0.39668831\n",
      "   0.47663898  0.45922709  0.30672553  0.31674905  0.46642189]]\n",
      "[[ 8.         64.          0.          9.         11.         10.\n",
      "  11.          3.          2.         10.          2.         13.\n",
      "   9.          0.38082974  0.40877612  0.4337111   0.49345555  0.36357362\n",
      "   0.41194693  0.48118191  0.36590012  0.49055809  0.33795075]]\n",
      "[[ 8.          8.          1.         15.         20.         12.\n",
      "  18.         20.          2.         18.          4.         15.\n",
      "  19.          0.34312458  0.49015266  0.3690477   0.49329699  0.45239641\n",
      "   0.43485277  0.37338699  0.40682121  0.48266585  0.48209679]]\n",
      "[[ 3.         32.          0.          7.          6.         15.\n",
      "  15.          9.         10.         20.          7.         19.\n",
      "   1.          0.4562317   0.3547306   0.30118674  0.32484073  0.43720875\n",
      "   0.43059296  0.34910359  0.3105215   0.47436775  0.46965883]]\n",
      "[[10.          8.          1.         19.         13.         17.\n",
      "  17.          1.          4.          8.         15.          6.\n",
      "  15.          0.33548831  0.3283271   0.49072854  0.42785164  0.31119788\n",
      "   0.42959326  0.43313308  0.4417778   0.35000099  0.3276101 ]]\n",
      "[[ 3.         16.          2.         17.          7.          3.\n",
      "  17.          3.          4.         11.          8.          6.\n",
      "  19.          0.43850063  0.44759754  0.3174643   0.39138938  0.32535482\n",
      "   0.32203287  0.43077917  0.4714281   0.49807957  0.42164552]]\n",
      "[[ 6.         16.          1.         17.          8.         16.\n",
      "   1.         19.         11.          6.         10.         16.\n",
      "  11.          0.32644016  0.39963501  0.49773202  0.40308814  0.38165164\n",
      "   0.49418698  0.4818675   0.42696466  0.34259683  0.34461536]]\n",
      "[[ 1.          8.          1.         10.         14.          3.\n",
      "   8.         16.          9.         17.          6.         19.\n",
      "  16.          0.35329069  0.42100383  0.43292272  0.32252947  0.31196869\n",
      "   0.46974208  0.33733896  0.47034863  0.4510177   0.47750993]]\n",
      "[[ 3.         64.          0.         13.         15.          5.\n",
      "  10.          4.          3.          5.          3.         10.\n",
      "  10.          0.41665515  0.44753041  0.43368833  0.44643449  0.39279585\n",
      "   0.44947999  0.35801682  0.32028566  0.38198088  0.47426907]]\n",
      "[[ 8.         16.          1.         12.         12.          9.\n",
      "  19.         18.         15.         17.         15.         13.\n",
      "   6.          0.49325195  0.35773174  0.48471678  0.42002516  0.36180301\n",
      "   0.31839538  0.36822851  0.48226088  0.43099122  0.42652451]]\n",
      "[[10.         32.          2.         12.          1.          8.\n",
      "  16.         17.         10.         13.         19.         14.\n",
      "  18.          0.43687882  0.37173726  0.46766954  0.38307005  0.42838801\n",
      "   0.41608644  0.45051246  0.45336904  0.40470422  0.49318446]]\n",
      "[[ 3.         64.          1.          8.         15.         13.\n",
      "   9.         15.         12.         11.         19.         17.\n",
      "  15.          0.40378385  0.30263266  0.4452087   0.34515526  0.35494374\n",
      "   0.32853087  0.3681024   0.31162077  0.40250835  0.46611105]]\n",
      "[[ 6.         32.          2.          7.          9.          8.\n",
      "  13.          7.          4.         11.          3.          1.\n",
      "   8.          0.44426659  0.32651657  0.38349898  0.30594241  0.32174115\n",
      "   0.45442353  0.31923592  0.31144137  0.33673167  0.49969126]]\n",
      "[[ 5.         16.          0.          1.          8.         18.\n",
      "  18.          8.          6.          8.         19.         13.\n",
      "  19.          0.49789539  0.40729294  0.47423508  0.42466749  0.44525544\n",
      "   0.37006796  0.49664039  0.34619845  0.38054596  0.47793351]]\n",
      "[[10.         32.          2.         17.          9.          1.\n",
      "  18.          9.          4.         18.          1.         18.\n",
      "  18.          0.41271841  0.45497109  0.30248233  0.39185654  0.3591715\n",
      "   0.30419961  0.37289738  0.48382195  0.44857825  0.43356392]]\n",
      "[[ 7.          8.          2.         10.         19.         11.\n",
      "  19.         17.          5.         14.          4.         19.\n",
      "   2.          0.40821107  0.37681128  0.43037846  0.37044966  0.36401995\n",
      "   0.38017133  0.43825502  0.3845052   0.42572164  0.4908352 ]]\n",
      "[[ 9.          8.          2.         13.          9.         14.\n",
      "  16.          7.         15.         19.          4.         16.\n",
      "   8.          0.45023798  0.35689147  0.41529341  0.44423356  0.30642489\n",
      "   0.31962978  0.41384977  0.46370671  0.4212784   0.40257274]]\n",
      "[[ 4.         32.          2.          9.          5.         16.\n",
      "   5.         11.         13.         19.          8.         19.\n",
      "  12.          0.37350818  0.3145461   0.42601971  0.31715053  0.38853593\n",
      "   0.35029928  0.38921844  0.33214352  0.33367585  0.41727617]]\n",
      "[[ 2.         16.          0.          1.         17.         12.\n",
      "  20.         13.          4.         15.         17.          6.\n",
      "  20.          0.35012876  0.38536149  0.36175472  0.35449542  0.46418335\n",
      "   0.40499912  0.39907924  0.4196196   0.42879662  0.38651191]]\n",
      "[[ 8.         64.          1.         15.          8.          6.\n",
      "   7.         11.          7.          9.         18.         20.\n",
      "   9.          0.42255296  0.36468404  0.48873205  0.37295811  0.40221864\n",
      "   0.48576944  0.38826016  0.40830359  0.33720104  0.42145173]]\n",
      "[[ 3.          8.          0.         20.         18.          6.\n",
      "   6.          3.         11.         15.         20.          6.\n",
      "  17.          0.49740454  0.32283611  0.46406418  0.49118267  0.35652318\n",
      "   0.47676027  0.38861001  0.42213712  0.49127533  0.46164439]]\n",
      "[[ 1.         16.          1.         18.          3.         15.\n",
      "   3.         20.         15.          4.          8.         19.\n",
      "  17.          0.32068665  0.36787475  0.44273455  0.49525154  0.39703852\n",
      "   0.43682015  0.33963218  0.31314812  0.38915899  0.41944725]]\n",
      "[[ 9.          8.          1.          8.          6.         19.\n",
      "   2.         10.          8.         19.         15.         20.\n",
      "  10.          0.49899817  0.38656814  0.4819362   0.32651941  0.33756204\n",
      "   0.37526772  0.42382069  0.39379722  0.46482669  0.44338987]]\n",
      "[[ 2.         64.          0.          8.          7.         15.\n",
      "   1.         19.         16.          1.          3.          5.\n",
      "  15.          0.30221347  0.47817741  0.40703896  0.32116605  0.39880605\n",
      "   0.31301011  0.32712633  0.44601162  0.3309928   0.49323212]]\n",
      "[[ 1.         64.          1.          8.          9.          8.\n",
      "  16.         14.         10.          4.         10.         15.\n",
      "   7.          0.48975249  0.35131403  0.36927607  0.30078838  0.36737197\n",
      "   0.45642442  0.38725007  0.37093508  0.34966977  0.37066147]]\n",
      "[[ 6.         64.          1.          7.         19.         20.\n",
      "  11.         17.          6.         20.         14.          3.\n",
      "   7.          0.41552913  0.34249508  0.38252361  0.49658835  0.3232766\n",
      "   0.46986547  0.48598836  0.35779538  0.43812496  0.43369421]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.         32.          1.          6.         14.         16.\n",
      "   9.          4.          2.         10.         12.         15.\n",
      "  20.          0.47268259  0.39389691  0.36796363  0.39170985  0.36460834\n",
      "   0.46023164  0.4757596   0.31542198  0.4340788   0.42361643]]\n",
      "[[ 4.         64.          1.         15.         13.          7.\n",
      "  15.         13.         14.          4.         13.          8.\n",
      "   7.          0.32773157  0.34854989  0.36152363  0.47132159  0.33516384\n",
      "   0.30225435  0.45215529  0.32254674  0.39155831  0.4861717 ]]\n",
      "[[ 2.         16.          1.         17.         13.          7.\n",
      "  11.          1.         13.          7.         13.         20.\n",
      "  10.          0.39678198  0.38191086  0.38624308  0.36179424  0.49804183\n",
      "   0.34704251  0.46389364  0.34387385  0.41407432  0.42812681]]\n",
      "[[ 3.          8.          2.         19.         15.          8.\n",
      "  19.          1.          3.          4.          7.          6.\n",
      "   7.          0.30504246  0.41027829  0.4384905   0.3138363   0.39677094\n",
      "   0.38883542  0.47035277  0.36981504  0.30086484  0.35117192]]\n",
      "[[ 9.         16.          1.         19.          6.          6.\n",
      "   4.         18.         10.         19.         12.         15.\n",
      "  16.          0.46385499  0.42729466  0.33582066  0.37221833  0.41760052\n",
      "   0.35880676  0.38940144  0.30808105  0.49158745  0.48079969]]\n",
      "[[10.          8.          1.          9.         12.         17.\n",
      "  19.         10.          1.          7.         10.          5.\n",
      "   5.          0.44215255  0.3071593   0.30416325  0.49939448  0.40120407\n",
      "   0.3675978   0.41240232  0.3045521   0.39464284  0.47626628]]\n",
      "[[10.         32.          0.         11.          6.         10.\n",
      "  15.          7.         17.         17.          9.          1.\n",
      "   1.          0.35832758  0.31175011  0.42387186  0.43203486  0.3343234\n",
      "   0.3480076   0.4241806   0.42992581  0.38443733  0.32983408]]\n",
      "[[ 9.         32.          1.          4.         16.         18.\n",
      "   1.          8.         15.          6.          1.          2.\n",
      "   5.          0.47514072  0.45882693  0.46609135  0.31721528  0.39347993\n",
      "   0.38182039  0.32072534  0.49614866  0.36147367  0.39519929]]\n",
      "[[ 9.         32.          1.         17.         15.          8.\n",
      "  16.         15.         19.          3.          4.         19.\n",
      "  16.          0.37216825  0.32840893  0.31416564  0.35183503  0.37076361\n",
      "   0.35024806  0.35291539  0.34828469  0.34749176  0.39839215]]\n",
      "[[ 9.          8.          1.          8.         20.         12.\n",
      "   8.          9.          1.         20.          6.          9.\n",
      "  13.          0.30097505  0.44467549  0.36897902  0.35832431  0.44758733\n",
      "   0.48220241  0.31661163  0.42673282  0.47693126  0.473155  ]]\n",
      "[[ 7.         64.          0.         20.         10.          2.\n",
      "   2.          6.          5.         14.         15.          6.\n",
      "  19.          0.32418025  0.32357318  0.39008774  0.44184662  0.47358797\n",
      "   0.30790921  0.39292992  0.3922803   0.30950289  0.30724339]]\n",
      "[[ 1.          8.          2.         11.         15.         20.\n",
      "   1.          1.          6.         10.         13.          8.\n",
      "  17.          0.41655939  0.40991527  0.46582707  0.42113413  0.30233892\n",
      "   0.3687366   0.3016373   0.33048355  0.45817568  0.32097446]]\n",
      "[[10.         32.          0.         17.          4.         13.\n",
      "   8.          8.         16.          5.         11.          5.\n",
      "   7.          0.35699575  0.47388401  0.42134425  0.47864151  0.49973384\n",
      "   0.41664387  0.42861527  0.46921807  0.44203682  0.30926055]]\n",
      "[[ 8.         16.          2.          7.          9.         18.\n",
      "   6.          5.         16.         10.          9.          2.\n",
      "  13.          0.32549349  0.46509109  0.34862579  0.35220525  0.37342294\n",
      "   0.49484723  0.36733221  0.36315101  0.33452566  0.44294152]]\n",
      "[[ 8.         64.          0.          9.         20.          6.\n",
      "  18.         15.          9.          9.         11.          3.\n",
      "  18.          0.47783551  0.48075608  0.49875233  0.40526168  0.45378008\n",
      "   0.32894966  0.33663189  0.42844514  0.47982993  0.49877294]]\n",
      "[[ 2.         32.          0.          4.         16.          6.\n",
      "   2.          5.          7.         10.          6.         20.\n",
      "  20.          0.3917325   0.32882642  0.38265459  0.39389585  0.47169709\n",
      "   0.46579554  0.47759566  0.47513345  0.48051439  0.31553061]]\n",
      "[[ 2.         16.          1.         12.          6.          7.\n",
      "  12.         19.         12.          4.         13.          3.\n",
      "  11.          0.36465718  0.37583079  0.35722646  0.40288536  0.33760355\n",
      "   0.46524659  0.45675382  0.43156726  0.4960198   0.32033625]]\n",
      "[[ 8.         32.          0.         10.          7.         11.\n",
      "  10.          9.          9.          6.          5.         16.\n",
      "  15.          0.37160713  0.4771865   0.34859069  0.36129052  0.46196201\n",
      "   0.30239342  0.36028267  0.45142638  0.38128288  0.41373505]]\n",
      "[[ 8.         16.          1.         11.         19.          3.\n",
      "  19.          4.         14.          7.         16.          8.\n",
      "  17.          0.42990189  0.41987995  0.30407873  0.38614663  0.47206091\n",
      "   0.35928222  0.4760212   0.34576322  0.42889625  0.34143832]]\n",
      "[ 4.          8.          1.          7.          8.          6.\n",
      "  7.         10.          8.         13.          6.          1.\n",
      " 17.          0.33131622  0.4032712   0.40289095  0.34169084  0.36344491\n",
      "  0.38637462  0.32798254  0.47208793  0.48940796  0.38899528]\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv('RUD_K_okada2_test3.csv')\n",
    "df = df.sample(frac=1, random_state=0)\n",
    "X_train = df.iloc[:, :4]\n",
    "y_train = df.iloc[:, 5]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "# 目的関数の定義\n",
    "def function(parameters):\n",
    "    print(parameters)\n",
    "    n_hidden = int(parameters[0, 0])\n",
    "    #n_drop = parameters[0, 1]\n",
    "    batch_size = int(parameters[0, 1])\n",
    "    optimizer = ['adam', 'sgd', 'rmsprop'][int(parameters[0, 2])]\n",
    "    #n_neurons = parameters[0, 4:n_hidden+4].astype(int)\n",
    "    n_neurons = parameters[0, 3:13].astype(int)\n",
    "    n_drop = parameters[0, 13:23]\n",
    "   # print(n_hidden,n_neurons, n_drop,batch_size,optimizer)\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(4,)))\n",
    "    for i in range(n_hidden):\n",
    "        model.add(layers.Dense(n_neurons[i], activation='relu'))\n",
    "        #leaky_relu\n",
    "        model.add(layers.Dropout(n_drop[i]))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_mae', patience=20)]\n",
    "\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        epochs=2000,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_split=0.1)\n",
    "    \n",
    "    evaluation = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    #print(evaluation[0])\n",
    "    return evaluation[0]\n",
    "    #score = model.evaluate(X_test, y_test)\n",
    "    #predictions = predict_function(model, X_test).flatten()\n",
    "    #predictions = model.predict(X_test).flatten()\n",
    "    #mse = mean_squared_error(y_test, predictions)\n",
    "    #return -mse\n",
    "\n",
    "# ハイパーパラメータの範囲とタイプの指定\n",
    "#max_layers = 10  # チューニングする最大層数\n",
    "bounds = [{'name': 'n_hidden', 'type': 'discrete', 'domain': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)},\n",
    "          #{'name': 'n_drop', 'type': 'continuous', 'domain': (0.3, 0.5)},\n",
    "          {'name': 'batch_size', 'type': 'discrete', 'domain': (8, 16, 32, 64)},\n",
    "          {'name': 'optimizer', 'type': 'discrete', 'domain': (0, 1, 2)}]\n",
    "for i in range(1, 11):\n",
    "    bounds += [{'name': 'n_neurons_{}'.format(i), 'type': 'discrete', 'domain': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
    "                                                                                 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)}]\n",
    "for j in range(1, 11):\n",
    "    bounds += [{'name': 'n_drop_{}'.format(j), 'type': 'continuous', 'domain': (0.3, 0.5)}]\n",
    "# ベイズ最適化の設定\n",
    "myRelt = BayesianOptimization(f=function,\n",
    "                              domain=bounds,\n",
    "                              #initinal_design_numdata=5,\n",
    "                              acquisition_type='EI')\n",
    "myRelt.run_optimization(max_iter=200)\n",
    "\n",
    "# チューニング結果の取得\n",
    "X = myRelt.X  # パラメータの履歴\n",
    "Y = myRelt.Y  # 目的関数の値の履歴\n",
    "\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)    #MSE\n",
    "\n",
    "#myRelt.plot_acquisition()\n",
    "#myRelt.plot_convergence()\n",
    "\n",
    "\n",
    "###最適なハイパーパラメータとそのときのMSEを表示\n",
    "#best_params = myRelt.X[np.argmin(myRelt.Y)]\n",
    "#best_mse = -np.min(myRelt.Y)\n",
    "#print(\"Best Parameters:\", best_params)\n",
    "#print(\"Best MSE:\", best_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.          8.          1.          7.          8.          6.\n",
      "  7.         10.          8.         13.          6.          1.\n",
      " 17.          0.33131622  0.4032712   0.40289095  0.34169084  0.36344491\n",
      "  0.38637462  0.32798254  0.47208793  0.48940796  0.38899528]\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.         64.          1.         12.         14.         11.\n",
      "  12.          7.         11.         12.         18.         17.\n",
      "  12.          0.36586361  0.43034085  0.31161965  0.40284859  0.37961416\n",
      "   0.32211273  0.44998657  0.3242915   0.45697603  0.40539276]]\n",
      "[[ 8.         16.          1.          3.          4.          8.\n",
      "   8.         19.         14.          5.         11.          9.\n",
      "  18.          0.40152856  0.4232649   0.34757546  0.40862891  0.33117228\n",
      "   0.45187495  0.47751985  0.4490898   0.31755843  0.36641201]]\n",
      "[[ 2.         32.          0.          2.         13.          3.\n",
      "   1.         19.         13.         13.          5.         10.\n",
      "  10.          0.38604442  0.31880358  0.38403581  0.4602604   0.42780384\n",
      "   0.39863789  0.40473458  0.35258477  0.37103598  0.36852921]]\n",
      "[[ 3.          8.          1.         10.         12.          6.\n",
      "   6.         20.          8.          6.         17.          6.\n",
      "   3.          0.46472527  0.31579581  0.38272169  0.40954     0.42621327\n",
      "   0.48468382  0.4692266   0.4645226   0.39867583  0.47765664]]\n",
      "[[ 3.         16.          0.          4.         11.         11.\n",
      "   3.          2.         12.          6.         12.          2.\n",
      "  12.          0.34870375  0.42265661  0.4387198   0.43999146  0.31652585\n",
      "   0.35008559  0.33909699  0.48045481  0.49366329  0.33827433]]\n",
      "[[10.         32.          0.         18.         19.         16.\n",
      "   7.          4.          7.          1.         17.         11.\n",
      "   9.          0.43265288  0.45156777  0.31510942  0.38242429  0.32560491\n",
      "   0.48086325  0.42086943  0.48125606  0.42779692  0.33876746]]\n",
      "[[ 5.         16.          1.         16.          1.          3.\n",
      "   7.         13.          2.         20.         10.         16.\n",
      "  16.          0.45881879  0.3825742   0.42956467  0.48469852  0.42484708\n",
      "   0.35796425  0.47342452  0.39136069  0.36250707  0.32569216]]\n",
      "[[ 7.          8.          0.          8.         11.          3.\n",
      "   9.         19.         14.          2.         19.         10.\n",
      "  13.          0.36438569  0.41690826  0.45299693  0.40627726  0.42796335\n",
      "   0.46763832  0.39340305  0.35720535  0.42184425  0.48897366]]\n",
      "[[ 6.         16.          1.          2.         14.         15.\n",
      "   3.          6.         19.          7.         14.          7.\n",
      "  15.          0.31350009  0.309124    0.4890715   0.42580761  0.30950482\n",
      "   0.41737851  0.43654865  0.44007122  0.36350264  0.44460837]]\n",
      "[[ 8.         16.          1.         15.          2.          5.\n",
      "   7.         14.         10.         19.          7.         11.\n",
      "  13.          0.47501939  0.36369195  0.41461762  0.40445434  0.41984309\n",
      "   0.3513      0.45537226  0.35338826  0.46487246  0.46282361]]\n",
      "[[ 1.         16.          0.         14.         17.          6.\n",
      "   1.          1.          1.          6.         13.          6.\n",
      "   9.          0.36979145  0.49535128  0.43909919  0.5         0.37857027\n",
      "   0.35193887  0.3         0.46872175  0.5         0.3       ]]\n",
      "[[ 1.         16.          0.         20.          7.          1.\n",
      "   1.          1.          1.         15.         20.         20.\n",
      "  11.          0.38504328  0.5         0.5         0.5         0.5\n",
      "   0.3262382   0.3         0.5         0.5         0.3       ]]\n",
      "[[ 1.   8.   0.  20.  17.   1.  17.   1.   1.  19.  20.  20.  17.   0.5\n",
      "   0.5  0.5  0.5  0.5  0.3  0.3  0.3  0.5  0.3]]\n",
      "[[10.  16.   0.  20.  20.   1.  15.   1.   1.   1.  20.  20.   3.   0.3\n",
      "   0.5  0.5  0.5  0.5  0.3  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.         16.          0.         18.         11.          4.\n",
      "   4.          1.          1.         20.         17.         13.\n",
      "  19.          0.5         0.49604399  0.46185355  0.5         0.43780111\n",
      "   0.34828423  0.3         0.3470684   0.49339333  0.3       ]]\n",
      "[[ 1.          8.          0.         20.          5.          1.\n",
      "   7.          2.          1.         20.         20.         20.\n",
      "  19.          0.5         0.47741147  0.5         0.5         0.5\n",
      "   0.3         0.32862823  0.33994527  0.46959041  0.3       ]]\n",
      "[[ 1.         32.          0.         16.         15.         18.\n",
      "   1.          1.          2.          7.         13.          1.\n",
      "  11.          0.5         0.49776854  0.3         0.39153668  0.3\n",
      "   0.5         0.30540252  0.5         0.5         0.3       ]]\n",
      "[[ 1.         16.          0.         10.         10.          7.\n",
      "   1.          1.          1.         18.         18.          1.\n",
      "   7.          0.3         0.5         0.39481664  0.5         0.3\n",
      "   0.5         0.3         0.5         0.5         0.3       ]]\n",
      "[[ 1.         16.          0.         20.         17.         19.\n",
      "   1.          1.          2.          6.         20.          1.\n",
      "  17.          0.5         0.5         0.3         0.3         0.3\n",
      "   0.5         0.33289898  0.5         0.5         0.3       ]]\n",
      "[[ 1.  16.   0.  14.  20.  15.   1.   1.   1.  13.   1.   1.  12.   0.5\n",
      "   0.5  0.3  0.3  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.  16.   0.  20.   8.  19.   1.   1.   1.  20.  19.  20.  11.   0.5\n",
      "   0.5  0.3  0.5  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.          8.          0.         20.         20.          1.\n",
      "   1.         12.          1.         20.         20.         20.\n",
      "  20.          0.5         0.5         0.3         0.5         0.49575649\n",
      "   0.5         0.3         0.3         0.5         0.3       ]]\n",
      "[[ 1.  16.   0.  20.   2.   8.   1.   1.   1.   4.  16.   1.  14.   0.5\n",
      "   0.3  0.5  0.5  0.5  0.3  0.3  0.3  0.5  0.3]]\n",
      "[[ 1.         32.          0.         20.         20.         20.\n",
      "   1.          1.         13.          1.         16.          1.\n",
      "   5.          0.5         0.5         0.3         0.5         0.3\n",
      "   0.5         0.37859013  0.3         0.5         0.3       ]]\n",
      "[[ 1.  16.   0.  20.   4.   1.   1.   1.   1.  20.   4.  20.  11.   0.5\n",
      "   0.5  0.5  0.5  0.5  0.5  0.3  0.3  0.5  0.3]]\n",
      "[[ 1.   8.   0.   2.  20.   1.   1.   1.   1.  12.  20.   1.  20.   0.5\n",
      "   0.3  0.5  0.5  0.5  0.3  0.3  0.3  0.5  0.3]]\n",
      "[[ 1.         16.          0.         17.         12.         13.\n",
      "   1.          1.          8.          7.         14.          1.\n",
      "   5.          0.3         0.5         0.3         0.49101666  0.3\n",
      "   0.5         0.3         0.5         0.5         0.3       ]]\n",
      "[[ 1.  16.   0.   6.  11.  18.   1.   1.   2.   1.  10.   1.   1.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.3  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.  16.   0.  20.   8.   4.   1.   5.   1.  20.  17.  10.   7.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.3  0.5  0.5  0.3  0.3]]\n",
      "[[ 1.  32.   0.  20.  17.  16.   1.   1.   8.   1.  11.   7.  14.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.   8.   0.  20.  20.  20.  20.   1.   1.  20.  20.  20.  20.   0.5\n",
      "   0.5  0.3  0.5  0.3  0.5  0.3  0.3  0.5  0.3]]\n",
      "[[ 1.  32.   0.  11.  12.   5.   1.   1.   9.   5.   8.   1.   5.   0.3\n",
      "   0.5  0.3  0.5  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.  16.   0.  20.  20.   3.   1.   1.  20.   1.   1.   1.  10.   0.5\n",
      "   0.5  0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  32.   0.  20.  20.   1.   1.  20.   1.  20.   1.   1.   1.   0.3\n",
      "   0.5  0.3  0.5  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.  32.   0.  20.  20.   1.   1.   1.  15.  20.   1.   1.   1.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.3  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.  32.   0.  10.  11.   1.   1.   1.   1.  20.   1.   1.   1.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.3  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.   8.   0.  20.  14.   1.  20.  20.   1.  20.  20.  20.  20.   0.5\n",
      "   0.5  0.3  0.5  0.3  0.5  0.3  0.3  0.5  0.3]]\n",
      "[[ 1.  32.   0.  20.  20.   6.   1.   6.  10.   9.  12.   1.  10.   0.5\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.  32.   0.  20.  20.  20.   1.  20.   1.   1.   1.   1.   1.   0.3\n",
      "   0.5  0.3  0.5  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[10.  32.   0.  20.  20.  20.   1.   1.  11.   1.   1.   1.   2.   0.3\n",
      "   0.5  0.3  0.5  0.3  0.3  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.         32.          0.         18.         17.         16.\n",
      "   1.          3.         10.          1.         20.          5.\n",
      "  11.          0.5         0.5         0.33927353  0.37788634  0.3\n",
      "   0.5         0.4416265   0.3         0.48525191  0.3144588 ]]\n",
      "[[ 1.         32.          0.         18.         11.          9.\n",
      "   1.          1.          4.         11.         13.          7.\n",
      "  12.          0.3         0.5         0.49638122  0.3         0.3\n",
      "   0.5         0.3         0.5         0.5         0.3       ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         32.          0.         20.         20.         18.\n",
      "   1.         20.          1.          5.         14.          3.\n",
      "   7.          0.38979149  0.5         0.37984584  0.428341    0.3\n",
      "   0.5         0.3         0.49682891  0.5         0.3       ]]\n",
      "[[ 1.         16.          0.         17.         14.          1.\n",
      "   1.          1.         12.         15.          9.          1.\n",
      "   7.          0.44330385  0.5         0.3         0.49769602  0.3\n",
      "   0.5         0.3         0.5         0.5         0.30167509]]\n",
      "[[ 1.         32.          0.         20.         20.         20.\n",
      "   5.          1.          3.          6.         16.         12.\n",
      "  18.          0.37275719  0.4761145   0.5         0.3239736   0.3\n",
      "   0.5         0.3         0.5         0.42179335  0.3       ]]\n",
      "[[ 1.         32.          0.         20.         20.         20.\n",
      "   7.          1.         12.          6.         20.          1.\n",
      "   8.          0.5         0.49932156  0.5         0.5         0.3\n",
      "   0.5         0.3         0.5         0.5         0.3       ]]\n",
      "[[ 1.          8.          0.         20.         17.          9.\n",
      "  13.          7.          1.         20.          7.         20.\n",
      "  20.          0.32046009  0.5         0.3         0.3         0.3\n",
      "   0.5         0.4935901   0.5         0.3         0.3       ]]\n",
      "[[ 1.         32.          0.         20.         20.          1.\n",
      "   1.          9.          1.         20.          3.         11.\n",
      "  16.          0.3         0.5         0.3         0.3         0.3\n",
      "   0.5         0.3631614   0.5         0.47780908  0.3       ]]\n",
      "[[ 1.  16.   0.  20.  20.   1.   1.   1.  11.  20.   5.  20.  20.   0.3\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   0.  20.  20.  12.   1.   7.   1.  20.   8.  20.  20.   0.3\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.5  0.3  0.3]]\n",
      "[[ 1.         32.          0.         20.         20.          1.\n",
      "   1.         20.          1.          3.          1.          1.\n",
      "  12.          0.3         0.5         0.3         0.3         0.3\n",
      "   0.5         0.47622214  0.5         0.5         0.42399893]]\n",
      "[[ 1.  16.   0.  20.  20.   1.   7.  20.   1.  20.   1.  20.  20.   0.5\n",
      "   0.5  0.3  0.3  0.3  0.5  0.3  0.3  0.5  0.3]]\n",
      "[[ 1.  16.   0.  20.  20.   1.  13.   1.   1.  20.   1.   9.  20.   0.3\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  32.   0.  20.  16.   9.   1.   5.   1.   1.   9.   9.  20.   0.5\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   0.  20.  20.   1.  20.   1.  20.  20.   1.  20.  20.   0.5\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.3  0.5  0.5]]\n",
      "[[ 1.  32.   0.  20.  20.   1.   1.   1.  20.  20.   1.   3.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.          8.          0.         20.         20.         20.\n",
      "  20.         20.          1.         20.          1.          1.\n",
      "  20.          0.39170899  0.5         0.3         0.3         0.3\n",
      "   0.5         0.5         0.3         0.5         0.3       ]]\n",
      "[[ 1.   8.   0.  20.  10.  19.  20.   1.   1.  20.   7.   1.  20.   0.3\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.  20.   1.  20.   1.  20.  20.   1.   1.  20.   0.5\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.  20.   9.   1.  20.   1.  20.   1.   1.  20.   0.3\n",
      "   0.5  0.3  0.5  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.         32.          0.         10.         10.         13.\n",
      "   1.          1.          1.          2.         20.          1.\n",
      "   4.          0.3         0.5         0.3         0.3         0.3\n",
      "   0.5         0.5         0.5         0.44631761  0.5       ]]\n",
      "[[ 1.  16.   0.  20.  19.   1.   1.   1.  20.   1.  12.   1.  20.   0.3\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.  20.  12.  20.  12.   1.  20.  10.   1.  20.   0.3\n",
      "   0.5  0.3  0.3  0.3  0.5  0.3  0.3  0.5  0.5]]\n",
      "[[ 1.  32.   0.  20.  20.   1.   1.   1.  20.  20.  20.   1.   1.   0.5\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.3  0.5  0.5]]\n",
      "[[ 1.         16.          0.         20.         20.         20.\n",
      "  12.         20.          1.          1.          8.          1.\n",
      "  20.          0.31904868  0.5         0.3         0.3         0.3\n",
      "   0.5         0.5         0.3         0.5         0.5       ]]\n",
      "[[ 1.  16.   0.  20.  20.   1.   1.  20.  20.  20.   1.   1.   9.   0.5\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.3  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.  20.  20.  20.  20.  20.  20.  20.  20.  20.   0.5\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.3  0.3  0.5]]\n",
      "[[ 1.   8.   0.  20.  20.  20.  20.   1.  20.  20.  20.   1.  20.   0.5\n",
      "   0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.         16.          0.         20.         20.         20.\n",
      "   1.         20.          1.         20.         20.          1.\n",
      "   1.          0.3         0.5         0.49890997  0.3         0.3\n",
      "   0.5         0.5         0.3         0.5         0.5       ]]\n",
      "[[ 1.  32.   0.  20.  18.  19.   1.  10.   7.   1.  20.  10.  14.   0.5\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   0.  20.   1.  20.  20.  20.   1.  20.  20.   1.   1.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.3  0.5  0.3]]\n",
      "[[ 1.   8.   0.  20.  10.  20.  20.  20.   1.  20.  20.   1.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.3  0.3  0.5]]\n",
      "[[ 1.   8.   0.  20.  20.  20.  20.  20.   1.  20.  11.   1.   2.   0.3\n",
      "   0.5  0.3  0.3  0.3  0.5  0.3  0.3  0.3  0.3]]\n",
      "[[ 1.  16.   0.  20.   1.   1.   9.   5.  20.  20.  17.   5.  20.   0.5\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.5  0.3  0.3]]\n",
      "[[ 1.  16.   0.  20.   7.  20.   9.  20.   1.   1.  20.   8.  19.   0.5\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  32.   0.  20.  14.  13.   8.  18.   1.  11.  20.   1.  13.   0.5\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.3]]\n",
      "[[ 1.  16.   0.  20.  15.   1.   1.   3.  20.  20.  15.  12.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.3  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.   1.   1.  20.  20.   1.  20.   1.   1.  10.   0.3\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.  12.   8.   9.  10.   8.  20.  13.   9.  16.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.   1.  20.  20.  20.   1.  20.   1.  20.  20.   0.5\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.   1.   1.   1.  20.   1.  20.   4.   1.   5.   0.5\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.3  0.3  0.5]]\n",
      "[[ 1.  32.   0.  20.  20.   9.   1.  13.  15.   1.  20.  11.  20.   0.3\n",
      "   0.5  0.3  0.5  0.3  0.5  0.5  0.5  0.5  0.3]]\n",
      "[[ 1.  16.   0.  20.  13.   9.  10.  10.   8.  20.  14.   9.  16.   0.3\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.  12.   1.   7.   7.  11.  20.   9.   9.  18.   0.5\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.3  0.3  0.3]]\n",
      "[[ 1.  16.   0.  20.  15.  15.   6.  10.   6.   7.  20.   9.  16.   0.5\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.   8.  20.  14.  20.   1.  20.   8.   1.   9.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.3]]\n",
      "[[ 1.   8.   0.  20.   1.  20.  20.   1.   1.  20.  20.   1.   9.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.   1.  20.  20.   1.  20.  20.  20.   1.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.3  0.3  0.3]]\n",
      "[[ 1.  32.   0.  20.  15.   1.  20.  20.   1.  20.   1.   1.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.3]]\n",
      "[[ 1.   8.   0.  20.   7.   5.  20.   1.  20.  20.  20.  20.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  32.   0.  20.  14.  20.  15.  20.   1.   1.  13.   1.   3.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.5  0.3  0.5  0.3]]\n",
      "[[ 1.  32.   0.  20.  20.  10.   1.   1.  20.   1.  14.  19.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.  16.   0.  20.   1.   4.  20.   1.   1.  20.   9.   1.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.  15.  13.   1.  20.   1.   1.   6.   1.   5.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.3  0.5  0.3]]\n",
      "[[ 1.  16.   0.  20.   1.   1.  20.  20.   1.  20.  20.   1.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.5  0.5  0.5  0.3]]\n",
      "[[ 1.  32.   0.  20.   1.   1.   1.  20.   1.   1.   5.   1.   1.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.   8.   0.  20.  20.  20.  20.  20.  20.   1.  20.   1.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.3]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          8.          0.         20.         20.         20.\n",
      "  20.         20.         20.         20.         20.          1.\n",
      "   1.          0.3         0.5         0.5         0.5         0.49813659\n",
      "   0.5         0.3         0.5         0.5         0.3       ]]\n",
      "[[ 1.  16.   0.  20.  20.  19.   1.   1.  20.   1.  20.   5.   7.   0.5\n",
      "   0.5  0.3  0.5  0.3  0.5  0.5  0.3  0.5  0.3]]\n",
      "[[ 1.  32.   0.  20.  20.   1.   1.  16.  20.   1.   7.  15.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.  20.  20.  20.  20.  20.  20.  14.   1.  20.   0.3\n",
      "   0.5  0.5  0.5  0.5  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.   1.  20.  20.  20.  20.   1.  20.   1.   4.   0.3\n",
      "   0.5  0.5  0.3  0.5  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.  20.   1.  20.  20.  20.  20.  20.   1.  20.   0.3\n",
      "   0.5  0.5  0.3  0.5  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.   1.   1.  20.  20.  20.  20.  20.   1.   1.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  32.   0.  20.  20.   1.  20.  20.  20.   1.  11.   1.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.   1.  20.  20.   1.  20.   1.  20.   1.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  32.   0.  20.   1.   1.  20.  20.   1.  20.  13.   1.   1.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.   1.  20.  20.  20.  20.  20.  20.   1.  13.   0.3\n",
      "   0.5  0.5  0.3  0.5  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.  20.   1.  20.  20.  20.   1.   1.   1.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.   7.   6.  20.   2.   1.  20.  10.  20.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  32.   0.  20.  20.   1.  20.  20.  20.  20.   2.   1.   3.   0.3\n",
      "   0.5  0.5  0.3  0.5  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.   1.  20.  20.  20.   1.   1.   1.   1.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.  32.   0.  20.   1.   1.  20.  20.   1.   1.   1.   1.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.  32.   0.  20.  20.   1.   1.  20.  20.   1.  15.   1.   1.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.   8.   0.  20.   1.   1.  20.  20.  20.   1.  20.   1.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.   1.  20.  20.   6.   1.   1.  14.   1.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.  32.   0.  20.  20.  20.  13.  20.   1.   1.   6.  20.  20.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.   8.   0.  20.  20.  20.  20.  20.  20.   1.   1.  20.  20.   0.3\n",
      "   0.5  0.5  0.5  0.5  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.  20.  20.   1.  20.   1.   1.   1.  20.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.   1.   1.  20.   1.  20.   1.   1.   1.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.   6.   1.   1.   1.  20.   1.   1.   1.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.   1.   1.  20.  20.   1.   1.   1.   1.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.  20.   1.   1.  20.  20.   1.   1.  20.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  32.   0.  20.  20.  20.  20.  20.  20.   1.  18.  19.  20.   0.3\n",
      "   0.3  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.  12.   1.   1.   1.  20.   1.   6.  15.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.  10.  12.  20.  11.  20.   1.   9.   1.  20.   0.3\n",
      "   0.5  0.5  0.5  0.5  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.   5.   7.   9.  19.  17.   1.  13.  17.  19.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.3]]\n",
      "[[ 1.  16.   1.  20.  10.   8.  12.  19.  13.   5.  13.  12.  17.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.   6.   1.  12.  18.  15.  12.   5.  10.  18.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.3]]\n",
      "[[ 1.  16.   0.  20.  14.  16.  12.  20.  11.   1.  20.  13.  17.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.  10.   7.  12.  17.  13.   8.  12.  10.  17.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   0.  20.   1.   1.  17.  13.  20.  20.   1.   9.  20.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   0.  20.  16.  15.  14.  20.  10.   1.  20.  11.  17.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.  11.   7.  12.  17.  11.   8.  12.   9.  16.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   1.  20.   8.   1.  15.  20.   7.  20.   3.   6.  13.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.  16.   0.  20.  12.  12.  11.  12.  18.   1.  20.  12.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   1.  20.  19.  20.  18.  20.   5.   1.  20.  11.  19.   0.3\n",
      "   0.3  0.5  0.5  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.  10.   6.  11.  17.  11.   9.  12.   8.  16.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.5  0.5  0.3  0.3]]\n",
      "[[ 1.  16.   0.  20.  12.  10.  10.  11.  20.   1.  20.  12.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.   8.   1.  16.  20.   7.  20.   3.   5.  13.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.5  0.5  0.5  0.3]]\n",
      "[[ 1.  16.   0.  20.  17.  20.  13.  20.   1.   1.  20.  12.  17.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   0.  20.  13.   9.  12.  20.   6.   8.  13.   8.  14.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  32.   0.  18.  20.  17.   8.   6.   8.   1.  15.  14.  12.   0.3\n",
      "   0.3  0.5  0.5  0.3  0.5  0.5  0.5  0.3  0.3]]\n",
      "[[ 1.   8.   0.  20.   1.  20.  20.  20.  20.   1.  20.  20.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.3]]\n",
      "[[ 1.  32.   1.  20.   1.   1.  20.  20.   1.  20.   7.  14.  20.   0.3\n",
      "   0.5  0.5  0.3  0.5  0.5  0.3  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   1.  20.   9.   1.  20.  20.  20.  20.  16.  17.  20.   0.3\n",
      "   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.3  0.3]]\n",
      "[[ 1.  16.   0.  20.  11.  20.  20.  13.  20.   1.  17.  13.  20.   0.3\n",
      "   0.5  0.5  0.3  0.5  0.5  0.5  0.5  0.3  0.3]]\n",
      "[[ 1.  32.   1.  20.   6.   1.   1.  20.  20.  20.   4.   1.   1.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.   1.  10.  20.  10.  11.  10.  12.  10.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  32.   1.  20.   1.   1.   8.  20.   1.  20.   1.   1.  15.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.3  0.5]]\n",
      "[[ 1.  32.   1.  20.  18.   1.   1.   1.  20.  20.   7.  13.  10.   0.3\n",
      "   0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  32.   0.  20.  20.   1.   1.  20.  20.  20.  15.  20.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   0.  20.  20.   1.   1.  20.  20.   1.   1.   1.  20.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  32.   1.  20.   1.   1.  20.  20.  20.  20.  11.   1.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   0.  20.  15.  16.   3.   1.   8.   1.  13.  13.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.   1.   1.  20.  20.   1.   1.  20.  20.   1.   1.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.   1.   1.  20.  20.  20.   1.   1.   1.   7.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  32.   1.  20.  20.   1.  12.  20.  20.  20.   4.   9.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   0.  20.  14.  20.  20.  20.   1.   1.   1.  20.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  32.   1.  20.   1.   1.   1.  20.   1.  20.  18.  20.  10.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.   1.   1.  20.  20.  20.  20.   1.  20.   1.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.   1.   1.  20.   1.  20.  20.   1.   1.   1.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.  20.  20.   1.  20.   1.  20.   1.  20.   1.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  32.   1.  20.   1.   1.  20.   1.  20.  20.   7.   1.   8.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  32.   1.  20.   1.   1.  20.  20.  20.  20.   1.   1.   1.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.3  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.  20.  10.   1.   1.  20.  20.  12.  20.  20.   0.3\n",
      "   0.5  0.5  0.3  0.5  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.   1.   1.   1.  20.  20.  20.  20.  20.   1.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  32.   1.  20.   1.   1.   1.  20.   1.  20.   1.  20.   1.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.  20.   4.   1.  20.  20.  20.  14.  20.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   1.  20.   1.   1.   1.   1.  20.  20.  17.  20.   7.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.   1.   1.  20.  20.  20.   1.   1.  20.  20.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.   1.   1.  20.   1.  20.  20.   1.  20.  20.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   0.   6.   7.   1.   1.   1.  20.   1.  13.   1.   1.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.   1.   1.  20.  20.  20.  20.  20.  20.  10.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.   1.   1.   1.  20.  20.  20.   1.  20.   1.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   1.  20.   9.   6.   1.  20.   1.  20.   9.  20.   8.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   1.  20.   1.   1.  10.  14.  20.  20.   7.   7.  12.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   1.  20.   1.   1.  10.  13.  20.  20.   7.   7.  11.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   0.  20.   6.   1.  10.  20.  15.  20.   6.   9.   9.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   1.  20.  20.  14.   1.  20.  20.   1.  20.  20.  18.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   1.  20.   5.   1.  11.  16.  16.  20.   7.   8.  11.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   1.  20.   5.   1.  10.  17.  14.  20.   7.   9.  10.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   0.  20.   1.   1.  20.  14.  20.  20.   6.   1.  12.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.3  0.3]]\n",
      "[[ 1.  16.   1.  20.   1.   1.   1.   9.  20.   1.  18.  10.  15.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   0.  20.  10.   1.   7.  18.  17.  18.   9.  11.  10.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   1.  20.   1.   1.  20.  20.   1.  20.   1.   3.   9.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.3  0.3]]\n",
      "[[ 1.  16.   0.  20.   1.   1.  13.   6.  20.  20.   9.   9.  15.   0.3\n",
      "   0.3  0.5  0.5  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  16.   0.  20.   1.   1.   1.  20.  20.  20.  14.  20.  19.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.   1.  20.   1.   1.   1.  20.  20.   1.   1.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  32.   0.  20.   9.   1.   1.   1.  20.   1.  11.   1.  12.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.   8.   1.  20.   8.   4.  17.  20.  20.  20.   7.   9.  14.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[10.  32.   0.  20.  20.  20.  12.  20.   1.   1.  20.  20.  20.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.         32.          0.         20.         19.         20.\n",
      "  14.         20.          3.          1.          1.         17.\n",
      "  18.          0.37547454  0.3         0.32486295  0.3         0.3\n",
      "   0.5         0.3         0.5         0.5         0.3       ]]\n",
      "[[ 1.   8.   0.  20.   1.  20.   1.  20.  20.   1.  20.  20.   1.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.          8.          0.         20.         17.         20.\n",
      "  16.         20.          1.          1.         16.         10.\n",
      "  16.          0.3         0.3         0.5         0.46560072  0.3\n",
      "   0.5         0.3         0.5         0.5         0.35306712]]\n",
      "[[ 1.  32.   0.  20.  20.  20.   1.   2.   4.   1.  20.   8.   4.   0.5\n",
      "   0.5  0.3  0.3  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   1.  20.  20.  20.   9.  20.   1.   1.   2.  20.  20.   0.3\n",
      "   0.3  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.3]]\n",
      "[[ 1.  32.   0.  20.  20.  18.   4.  17.  20.   1.  16.   9.  17.   0.3\n",
      "   0.5  0.3  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  16.   1.  20.  13.  20.   1.  20.   1.   1.  13.   1.   1.   0.3\n",
      "   0.5  0.5  0.3  0.5  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  32.   0.  20.   1.   1.  20.  20.  20.   1.  18.   1.   1.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.   8.   0.  20.   1.  20.  20.   1.  20.  20.  20.  20.   1.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  32.   0.  20.   1.   1.   1.  20.  20.   1.  15.  20.   1.   0.3\n",
      "   0.5  0.5  0.5  0.3  0.5  0.5  0.5  0.5  0.5]]\n",
      "[[ 1.  32.   1.  20.  12.   9.   8.  20.   1.   8.   9.   1.   7.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[[ 1.  32.   0.  20.   1.   1.   1.  20.  20.   1.  20.   1.   1.   0.3\n",
      "   0.5  0.3  0.5  0.3  0.5  0.5  0.5  0.3  0.5]]\n",
      "[[ 1.  32.   1.   1.   1.   1.  20.   1.   1.   1.   5.   1.   8.   0.3\n",
      "   0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.5]]\n",
      "[ 1.   8.   0.  20.  20.  20.  20.  20.  20.   1.  20.   1.  20.   0.3\n",
      "  0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.3]\n",
      "369.692138671875\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv('RUD_K_okada2_test3.csv')\n",
    "df = df.sample(frac=1, random_state=0)\n",
    "X_train = df.iloc[:, :4]\n",
    "y_train = df.iloc[:, 4]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "# 目的関数の定義\n",
    "def function(parameters):\n",
    "    print(parameters)\n",
    "    n_hidden = int(parameters[0, 0])\n",
    "    #n_drop = parameters[0, 1]\n",
    "    batch_size = int(parameters[0, 1])\n",
    "    optimizer = ['adam', 'rmsprop'][int(parameters[0, 2])]\n",
    "    #n_neurons = parameters[0, 4:n_hidden+4].astype(int)\n",
    "    n_neurons = parameters[0, 3:13].astype(int)\n",
    "    n_drop = parameters[0, 13:23]\n",
    "   # print(n_hidden,n_neurons, n_drop,batch_size,optimizer)\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(4,)))\n",
    "    for i in range(n_hidden):\n",
    "        model.add(layers.Dense(n_neurons[i], activation='relu'))\n",
    "        #leaky_relu\n",
    "        model.add(layers.Dropout(n_drop[i]))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_mae', patience=20)]\n",
    "\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        epochs=2000,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_split=0.1)\n",
    "    \n",
    "    evaluation = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    #print(evaluation[0])\n",
    "    return evaluation[0]\n",
    "    #score = model.evaluate(X_test, y_test)\n",
    "    #predictions = predict_function(model, X_test).flatten()\n",
    "    #predictions = model.predict(X_test).flatten()\n",
    "    #mse = mean_squared_error(y_test, predictions)\n",
    "    #return -mse\n",
    "\n",
    "# ハイパーパラメータの範囲とタイプの指定\n",
    "#max_layers = 10  # チューニングする最大層数\n",
    "bounds = [{'name': 'n_hidden', 'type': 'discrete', 'domain': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)},\n",
    "          #{'name': 'n_drop', 'type': 'continuous', 'domain': (0.3, 0.5)},\n",
    "          {'name': 'batch_size', 'type': 'discrete', 'domain': (8, 16, 32, 64)},\n",
    "          {'name': 'optimizer', 'type': 'discrete', 'domain': (0, 1)}]\n",
    "for i in range(1, 11):\n",
    "    bounds += [{'name': 'n_neurons_{}'.format(i), 'type': 'discrete', 'domain': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
    "                                                                                 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)}]\n",
    "for j in range(1, 11):\n",
    "    bounds += [{'name': 'n_drop_{}'.format(j), 'type': 'continuous', 'domain': (0.3, 0.5)}]\n",
    "# ベイズ最適化の設定\n",
    "myRelt = BayesianOptimization(f=function,\n",
    "                              domain=bounds,\n",
    "                              #initinal_design_numdata=5,\n",
    "                              acquisition_type='EI')\n",
    "myRelt.run_optimization(max_iter=200)\n",
    "\n",
    "# チューニング結果の取得\n",
    "X = myRelt.X  # パラメータの履歴\n",
    "Y = myRelt.Y  # 目的関数の値の履歴\n",
    "\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)    #MSE\n",
    "\n",
    "#myRelt.plot_acquisition()\n",
    "#myRelt.plot_convergence()\n",
    "\n",
    "\n",
    "###最適なハイパーパラメータとそのときのMSEを表示\n",
    "#best_params = myRelt.X[np.argmin(myRelt.Y)]\n",
    "#best_mse = -np.min(myRelt.Y)\n",
    "#print(\"Best Parameters:\", best_params)\n",
    "#print(\"Best MSE:\", best_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   8.   0.  20.  20.  20.  20.  20.  20.   1.  20.   1.  20.   0.3\n",
      "  0.5  0.5  0.3  0.3  0.5  0.3  0.5  0.5  0.3]\n",
      "369.692138671875\n"
     ]
    }
   ],
   "source": [
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  64.   0.  20.  20.   8.  20.  20.  20.   1.  15.   1.  20.   0.3\n",
      "  0.5  0.5  0.5  0.5  0.3  0.5  0.3  0.5  0.3]\n",
      "1.851058006286621\n"
     ]
    }
   ],
   "source": [
    "#関節角度\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  16.   2.   4.  17.   1.  20.   6.   1.  20.  20.   5.   1.   0.3\n",
      "  0.3  0.3  0.3  0.5  0.3  0.3  0.3  0.5  0.5]\n",
      "1.9939818382263184\n"
     ]
    }
   ],
   "source": [
    "#J_test4\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.          8.          1.         17.          7.         11.\n",
      " 14.          9.          6.          5.          2.          3.\n",
      "  9.          0.31260456  0.36782765  0.47810552  0.34235326  0.47031869\n",
      "  0.4469557   0.45948776  0.4922093   0.4158464   0.39217474]\n",
      "3.50406551361084\n"
     ]
    }
   ],
   "source": [
    "#J_test4\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.         32.          0.          1.          7.          5.\n",
      "  1.         10.          5.          7.          0.33866105  0.5\n",
      "  0.3         0.3         0.3         0.3         0.44535237]\n",
      "9.812644030660067e-09\n"
     ]
    }
   ],
   "source": [
    "#J_test3\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.  16.   0.   7.   1.   4.   5.   6.   1.   8.   0.3  0.3  0.5  0.3\n",
      "  0.3  0.3  0.5]\n",
      "9.848389659339318e-09\n"
     ]
    }
   ],
   "source": [
    "#J_test2\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.  16.   2.   1.   9.   2.  10.   5.  10.   7.   0.5  0.3  0.3  0.3\n",
      "  0.3  0.3  0.3]\n",
      "9.81240066977307e-09\n"
     ]
    }
   ],
   "source": [
    "#J_test\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         16.          1.         10.          6.          1.\n",
      "  1.         10.         10.          2.          0.5         0.5\n",
      "  0.3         0.46716815  0.5         0.3         0.40617582]\n",
      "2.0344951152801514\n"
     ]
    }
   ],
   "source": [
    "#K\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         32.          1.          6.         10.          4.\n",
      "  3.          3.          5.          2.          0.47557927  0.45322611\n",
      "  0.45634483  0.37157877  0.38722204  0.45363192  0.31898197]\n",
      "2.1508476734161377\n"
     ]
    }
   ],
   "source": [
    "#K\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   8.   2.  10.   7.  10.   8.   2.   4.   1.   0.5  0.5  0.3  0.3\n",
      "  0.3  0.3  0.3]\n",
      "0.0002846872666850686\n"
     ]
    }
   ],
   "source": [
    "#D\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.         8.         2.         3.         1.         5.\n",
      " 3.         1.         7.         2.         0.40331947 0.33508588\n",
      " 0.35694392 0.34865551 0.39585452 0.3        0.40922117]\n",
      "6.28968654936557e-09\n"
     ]
    }
   ],
   "source": [
    "#J\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験参加者共有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隠れ層のチューニングあり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, fmin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "#ニューロン数を20にするなどしてパラメータ数を多くするとALLATRIBUTEERRORが発生する\n",
    "#hp.choiceを使ったパラメータ値のprint(best)による出力は、選択した数値ではなくあくまでインデックスの値を返すため+1する必要がある\n",
    "hyperopt_parameters = {\n",
    "    'n_hidden': hp.choice('n_hidden',[1, 2, 3, 4, 5]),\n",
    "    #'n_neurons': hp.choice('n_neurons', [[hp.quniform(f'n_neurons_{neuron}', 1, 20, 1) for neuron in range(10)]]),\n",
    "    'n_neurons': hp.choice('n_neurons', [[hp.choice(f'n_neurons_{neuron}',\n",
    "                                                    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) for neuron in range(10)]]),\n",
    "    'n_drop': hp.choice('n_drop', [[hp.uniform(f'n_drop_{drop}', 0.3, 0.5) for drop in range(10)]]),\n",
    "    'optimizer': hp.choice('optimizer', ['Adam', 'SGD', 'RMSprop']),\n",
    "    'batch_size': hp.choice('batch_size', [8, 16, 32, 64])\n",
    "    #'epochs': hp.choice('epochs', [100, 250, 500, 1000, 2000]),\n",
    "    #'learning_rate': hp.uniform('learning_rate', 0.00001, 0.01),\n",
    "}\n",
    "\n",
    "\n",
    "# 静止状態\n",
    "\n",
    "df = pd.read_csv('KCB2.csv')\n",
    "df = df.sample(frac=1, random_state=0)\n",
    "X_train = df.iloc[:,:6]\n",
    "y_train = df.iloc[:,6]\n",
    "df2 = pd.read_csv('okada_K2.csv')\n",
    "df2 = df2.sample(frac=1, random_state=0)\n",
    "X_test = df2.iloc[:,:6]\n",
    "y_test = df2.iloc[:,6]\n",
    "\n",
    "\n",
    "def function(args):\n",
    "    #n_hidden = int(args['n_hidden'])\n",
    "    n_hidden = args['n_hidden']\n",
    "    n_drop = args['n_drop']\n",
    "    batch_size = int(args['batch_size'])\n",
    "    optimizer = args['optimizer']\n",
    "    n_neurons = args['n_neurons']\n",
    "    print(args['n_hidden'],args['n_neurons'],args['n_drop'],args['batch_size'],args['optimizer'])\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(6,)))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(layers.Dense(n_neurons[layer], activation='relu'))\n",
    "        model.add(layers.Dropout(n_drop[layer]))\n",
    "        #print(args['nneurons'][layer],args['ndrop'][layer])\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    #model.summary()\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics='mae')\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor = 'val_mae', patience = 20)]\n",
    "    \n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              epochs=1000,\n",
    "              batch_size=batch_size,\n",
    "              verbose=0,\n",
    "              callbacks=callbacks,\n",
    "              validation_split=0.1) # 検証用のデータ\n",
    "    \n",
    "    evaluation = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    return evaluation[0]\n",
    "\n",
    "#for i in tqdm(range(200)):\n",
    " #   trials = Trials()\n",
    "  #  best = fmin(function,\n",
    "   #             hyperopt_parameters,\n",
    "    #            algo=tpe.suggest,\n",
    "     #           max_evals=1,\n",
    "      #          trials=trials,\n",
    "       #         verbose=0)\n",
    "    \n",
    "trials = Trials()\n",
    "best = fmin(function,\n",
    "            hyperopt_parameters,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=200,\n",
    "            trials=trials,\n",
    "            verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 0, 'epochs': 2, 'l1_drop': 0.14296488874197943, 'l1_out': 3, 'l2_drop': 0.1662930115272234, 'l2_out': 5, 'optimizer': 2}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP-EIによる最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 静止状態\n",
    "df = pd.read_csv('KCB2.csv')\n",
    "df = df.sample(frac=1, random_state=0)\n",
    "X_train = df.iloc[:,:6]\n",
    "y_train = df.iloc[:,6]\n",
    "df2 = pd.read_csv('okada_K2.csv')\n",
    "df2 = df2.sample(frac=1, random_state=0)\n",
    "X_test = df2.iloc[:,:6]\n",
    "y_test = df2.iloc[:,6]\n",
    "\n",
    "# 目的関数の定義\n",
    "def function(parameters):\n",
    "    #print(parameters)\n",
    "    n_hidden = int(parameters[0, 0])\n",
    "    #n_drop = parameters[0, 1]\n",
    "    batch_size = int(parameters[0, 1])\n",
    "    optimizer = ['adam', 'sgd', 'rmsprop'][int(parameters[0, 2])]\n",
    "    #n_neurons = parameters[0, 4:n_hidden+4].astype(int)\n",
    "    n_neurons = parameters[0, 3:13].astype(int)\n",
    "    n_drop = parameters[0, 13:23]\n",
    "   # print(n_hidden,n_neurons, n_drop,batch_size,optimizer)\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(6,)))\n",
    "    for i in range(n_hidden):\n",
    "        model.add(layers.Dense(n_neurons[i], activation='relu'))\n",
    "        model.add(layers.Dropout(n_drop[i]))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_mae', patience=20)]\n",
    "\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        epochs=2000,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_split=0.1)\n",
    "    \n",
    "    evaluation = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    print(evaluation[0])\n",
    "    return evaluation[0]\n",
    "    #score = model.evaluate(X_test, y_test)\n",
    "    #predictions = predict_function(model, X_test).flatten()\n",
    "    #predictions = model.predict(X_test).flatten()\n",
    "    #mse = mean_squared_error(y_test, predictions)\n",
    "    #return -mse\n",
    "\n",
    "# ハイパーパラメータの範囲とタイプの指定\n",
    "#max_layers = 10  # チューニングする最大層数\n",
    "bounds = [{'name': 'n_hidden', 'type': 'discrete', 'domain': (1, 2, 3, 4, 5)},\n",
    "          #{'name': 'n_drop', 'type': 'continuous', 'domain': (0.3, 0.5)},\n",
    "          {'name': 'batch_size', 'type': 'discrete', 'domain': (8, 16, 32, 64)},\n",
    "          {'name': 'optimizer', 'type': 'discrete', 'domain': (0, 1, 2)}]\n",
    "for i in range(1, 11):\n",
    "    bounds += [{'name': 'n_neurons_{}'.format(i), 'type': 'discrete', 'domain': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)}]\n",
    "for j in range(1, 11):\n",
    "    bounds += [{'name': 'n_drop_{}'.format(j), 'type': 'continuous', 'domain': (0.3, 0.5)}]\n",
    "# ベイズ最適化の設定\n",
    "myRelt = BayesianOptimization(f=function,\n",
    "                              domain=bounds,\n",
    "                              #initinal_design_numdata=5,\n",
    "                              acquisition_type='EI')\n",
    "myRelt.run_optimization(max_iter=200)\n",
    "\n",
    "# チューニング結果の取得\n",
    "X = myRelt.X  # パラメータの履歴\n",
    "Y = myRelt.Y  # 目的関数の値の履歴\n",
    "\n",
    "print(myRelt.x_opt)     #最適なハイパーパラメータ\n",
    "print(myRelt.fx_opt)    #MSE\n",
    "\n",
    "#myRelt.plot_acquisition()\n",
    "#myRelt.plot_convergence()\n",
    "\n",
    "\n",
    "###最適なハイパーパラメータとそのときのMSEを表示\n",
    "#best_params = myRelt.X[np.argmin(myRelt.Y)]\n",
    "#best_mse = -np.min(myRelt.Y)\n",
    "#print(\"Best Parameters:\", best_params)\n",
    "#print(\"Best MSE:\", best_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
